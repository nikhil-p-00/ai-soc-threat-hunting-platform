.env.example:2:VT_API_KEY=YOUR_VT_API_KEY_HERE
detection\sysmon_rules.py:139:                                if vtres.get("error") == "no_api_key":
detection\sysmon_rules.py:140:                                    vt_info = " | VT: no_api_key"
Sysmon\Eula.txt:28:Please be aware that, similar to other debug tools that capture “process state” information, files saved by Sysinternals tools may include personally identifiable or other sensitive information (such as usernames, passwords, paths to files accessed, and paths to registry accessed). By using this software, you acknowledge that you are aware of this and take sole responsibility for any personally identifiable or other sensitive information provided to Microsoft or any other party through your use of the software.
Sysmon\sysmonconfig-export.xml:97:			<Image condition="is">C:\Windows\System32\TokenBrokerCookies.exe</Image> <!--Windows: SSO sign-in assistant for MicrosoftOnline.com-->
Sysmon\sysmonconfig-export.xml:352:			<DestinationPort name="VNC" condition="is">5800</DestinationPort> <!--VNC protocol: Monitor admin connections, often insecure, using hard-coded admin password-->
Sysmon\sysmonconfig-export.xml:353:			<DestinationPort name="VNC" condition="is">5900</DestinationPort> <!--VNC protocol Monitor admin connections, often insecure, using hard-coded admin password-->
Sysmon\sysmonconfig-export.xml:655:			<TargetObject condition="begin with">HKLM\SYSTEM\CurrentControlSet\Control\SecurityProviders</TargetObject> <!--Windows: Changes to WDigest-UseLogonCredential for password scraping [ https://www.trustedsec.com/april-2015/dumping-wdigest-creds-with-meterpreter-mimikatzkiwi-in-windows-8-1/ ] -->
Sysmon\sysmonconfig-export.xml:700:			<TargetObject name="T1088" condition="end with">HKLM\Software\Microsoft\Windows\CurrentVersion\Policies\System\LocalAccountTokenFilterPolicy</TargetObject> <!--Detect: UAC Tampering | Credit @ion-storm -->
Sysmon\sysmonconfig-export.xml:856:				<!-- Password or Credential Dumpers -->
tools\vt_lookup.py:5:VT_API_KEY = os.environ.get("VT_API_KEY")  # read from environment
tools\vt_lookup.py:8:HEADERS = {"x-apikey": VT_API_KEY} if VT_API_KEY else {}
tools\vt_lookup.py:15:    if not VT_API_KEY:
tools\vt_lookup.py:16:        return {"success": False, "error": "no_api_key"}
venv\Lib\site-packages\click\core.py:226:    :param token_normalize_func: an optional function that is used to
venv\Lib\site-packages\click\core.py:227:                                 normalize tokens (options, choices,
venv\Lib\site-packages\click\core.py:242:        Click 9.0. ``args`` will contain remaining unparsed tokens.
venv\Lib\site-packages\click\core.py:265:        ``token_normalize_func`` parameters.
venv\Lib\site-packages\click\core.py:288:        token_normalize_func: t.Callable[[str], str] | None = None,
venv\Lib\site-packages\click\core.py:394:        if token_normalize_func is None and parent is not None:
venv\Lib\site-packages\click\core.py:395:            token_normalize_func = parent.token_normalize_func
venv\Lib\site-packages\click\core.py:397:        #: An optional normalization function for tokens.  This is
venv\Lib\site-packages\click\core.py:399:        self.token_normalize_func: t.Callable[[str], str] | None = token_normalize_func
venv\Lib\site-packages\click\core.py:449:            " 'args' will contain remaining unparsed tokens.",
venv\Lib\site-packages\click\core.py:1918:        if cmd is None and ctx.token_normalize_func is not None:
venv\Lib\site-packages\click\core.py:1919:            cmd_name = ctx.token_normalize_func(cmd_name)
venv\Lib\site-packages\click\core.py:2668:        will be hidden from the user. This is useful for password input.
venv\Lib\site-packages\click\decorators.py:404:def password_option(*param_decls: str, **kwargs: t.Any) -> t.Callable[[FC], FC]:
venv\Lib\site-packages\click\decorators.py:405:    """Add a ``--password`` option which prompts for a password, hiding
venv\Lib\site-packages\click\decorators.py:409:        value ``"--password"``.
venv\Lib\site-packages\click\decorators.py:413:        param_decls = ("--password",)
venv\Lib\site-packages\click\parser.py:121:    if ctx is None or ctx.token_normalize_func is None:
venv\Lib\site-packages\click\parser.py:124:    return f"{prefix}{ctx.token_normalize_func(opt)}"
venv\Lib\site-packages\click\shell_completion.py:469:    incomplete escape sequence and uses the partial token as-is.
venv\Lib\site-packages\click\shell_completion.py:492:        for token in lex:
venv\Lib\site-packages\click\shell_completion.py:493:            out.append(token)
venv\Lib\site-packages\click\shell_completion.py:496:        # the partial token as-is. The quote or escape character is in
venv\Lib\site-packages\click\shell_completion.py:497:        # lex.state, not lex.token.
venv\Lib\site-packages\click\shell_completion.py:498:        out.append(lex.token)
venv\Lib\site-packages\click\types.py:293:        By default uses :meth:`Context.token_normalize_func` and if not case
venv\Lib\site-packages\click\types.py:300:        if ctx is not None and ctx.token_normalize_func is not None:
venv\Lib\site-packages\click\types.py:301:            normed_value = ctx.token_normalize_func(normed_value)
venv\Lib\site-packages\click\__init__.py:26:from .decorators import password_option as password_option
venv\Lib\site-packages\dateutil\parser\_parser.py:74:        self.tokenstack = []
venv\Lib\site-packages\dateutil\parser\_parser.py:77:    def get_token(self):
venv\Lib\site-packages\dateutil\parser\_parser.py:79:        This function breaks the time string into lexical units (tokens), which
venv\Lib\site-packages\dateutil\parser\_parser.py:87:        any dot-separated strings before breaking it into tokens; as such, this
venv\Lib\site-packages\dateutil\parser\_parser.py:88:        function maintains a "token stack", for when the ambiguous context
venv\Lib\site-packages\dateutil\parser\_parser.py:89:        demands that multiple tokens be parsed at once.
venv\Lib\site-packages\dateutil\parser\_parser.py:91:        if self.tokenstack:
venv\Lib\site-packages\dateutil\parser\_parser.py:92:            return self.tokenstack.pop(0)
venv\Lib\site-packages\dateutil\parser\_parser.py:95:        token = None
venv\Lib\site-packages\dateutil\parser\_parser.py:99:            # We only realize that we've reached the end of a token when we
venv\Lib\site-packages\dateutil\parser\_parser.py:100:            # find a character that's not part of the current token - since
venv\Lib\site-packages\dateutil\parser\_parser.py:101:            # that character may be part of the next token, it's stored in the
venv\Lib\site-packages\dateutil\parser\_parser.py:114:                # First character of the token - determines if we're starting
venv\Lib\site-packages\dateutil\parser\_parser.py:116:                token = nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:122:                    token = ' '
venv\Lib\site-packages\dateutil\parser\_parser.py:123:                    break  # emit token
venv\Lib\site-packages\dateutil\parser\_parser.py:125:                    break  # emit token
venv\Lib\site-packages\dateutil\parser\_parser.py:131:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:133:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:137:                    break  # emit token
venv\Lib\site-packages\dateutil\parser\_parser.py:142:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:143:                elif nextchar == '.' or (nextchar == ',' and len(token) >= 2):
venv\Lib\site-packages\dateutil\parser\_parser.py:144:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:148:                    break  # emit token
venv\Lib\site-packages\dateutil\parser\_parser.py:151:                # parsing, and the tokens will be broken up later.
venv\Lib\site-packages\dateutil\parser\_parser.py:154:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:155:                elif self.isnum(nextchar) and token[-1] == '.':
venv\Lib\site-packages\dateutil\parser\_parser.py:156:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:160:                    break  # emit token
venv\Lib\site-packages\dateutil\parser\_parser.py:163:                # break up the tokens later.
venv\Lib\site-packages\dateutil\parser\_parser.py:165:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:166:                elif self.isword(nextchar) and token[-1] == '.':
venv\Lib\site-packages\dateutil\parser\_parser.py:167:                    token += nextchar
venv\Lib\site-packages\dateutil\parser\_parser.py:171:                    break  # emit token
venv\Lib\site-packages\dateutil\parser\_parser.py:173:        if (state in ('a.', '0.') and (seenletters or token.count('.') > 1 or
venv\Lib\site-packages\dateutil\parser\_parser.py:174:                                       token[-1] in '.,')):
venv\Lib\site-packages\dateutil\parser\_parser.py:175:            l = self._split_decimal.split(token)
venv\Lib\site-packages\dateutil\parser\_parser.py:176:            token = l[0]
venv\Lib\site-packages\dateutil\parser\_parser.py:179:                    self.tokenstack.append(tok)
venv\Lib\site-packages\dateutil\parser\_parser.py:181:        if state == '0.' and token.count('.') == 0:
venv\Lib\site-packages\dateutil\parser\_parser.py:182:            token = token.replace(',', '.')
venv\Lib\site-packages\dateutil\parser\_parser.py:184:        return token
venv\Lib\site-packages\dateutil\parser\_parser.py:190:        token = self.get_token()
venv\Lib\site-packages\dateutil\parser\_parser.py:191:        if token is None:
venv\Lib\site-packages\dateutil\parser\_parser.py:194:        return token
venv\Lib\site-packages\dateutil\parser\_parser.py:619:            ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the
venv\Lib\site-packages\dateutil\parser\_parser.py:621:            a tuple containing the fuzzy tokens.
venv\Lib\site-packages\dateutil\parser\_parser.py:640:        res, skipped_tokens = self._parse(timestr, **kwargs)
venv\Lib\site-packages\dateutil\parser\_parser.py:656:        if kwargs.get('fuzzy_with_tokens', False):
venv\Lib\site-packages\dateutil\parser\_parser.py:657:            return ret, skipped_tokens
venv\Lib\site-packages\dateutil\parser\_parser.py:664:                     "tzname", "tzoffset", "ampm","any_unused_tokens"]
venv\Lib\site-packages\dateutil\parser\_parser.py:667:               fuzzy_with_tokens=False):
venv\Lib\site-packages\dateutil\parser\_parser.py:694:        :param fuzzy_with_tokens:
venv\Lib\site-packages\dateutil\parser\_parser.py:703:                >>> parse("Today is January 1, 2047 at 8:21:00AM", fuzzy_with_tokens=True)
venv\Lib\site-packages\dateutil\parser\_parser.py:707:        if fuzzy_with_tokens:
venv\Lib\site-packages\dateutil\parser\_parser.py:719:        l = _timelex.split(timestr)         # Splits the timestr into tokens
venv\Lib\site-packages\dateutil\parser\_parser.py:739:                    # Numeric token
venv\Lib\site-packages\dateutil\parser\_parser.py:740:                    i = self._parse_numeric_token(l, i, info, ymd, res, fuzzy)
venv\Lib\site-packages\dateutil\parser\_parser.py:869:        if fuzzy_with_tokens:
venv\Lib\site-packages\dateutil\parser\_parser.py:870:            skipped_tokens = self._recombine_skipped(l, skipped_idxs)
venv\Lib\site-packages\dateutil\parser\_parser.py:871:            return res, tuple(skipped_tokens)
venv\Lib\site-packages\dateutil\parser\_parser.py:875:    def _parse_numeric_token(self, tokens, idx, info, ymd, res, fuzzy):
venv\Lib\site-packages\dateutil\parser\_parser.py:876:        # Token is a number
venv\Lib\site-packages\dateutil\parser\_parser.py:877:        value_repr = tokens[idx]
venv\Lib\site-packages\dateutil\parser\_parser.py:881:            six.raise_from(ValueError('Unknown numeric token'), e)
venv\Lib\site-packages\dateutil\parser\_parser.py:885:        len_l = len(tokens)
venv\Lib\site-packages\dateutil\parser\_parser.py:890:             (tokens[idx + 1] != ':' and
venv\Lib\site-packages\dateutil\parser\_parser.py:891:              info.hms(tokens[idx + 1]) is None))):
venv\Lib\site-packages\dateutil\parser\_parser.py:893:            s = tokens[idx]
venv\Lib\site-packages\dateutil\parser\_parser.py:899:        elif len_li == 6 or (len_li > 6 and tokens[idx].find('.') == 6):
venv\Lib\site-packages\dateutil\parser\_parser.py:901:            s = tokens[idx]
venv\Lib\site-packages\dateutil\parser\_parser.py:903:            if not ymd and '.' not in tokens[idx]:
venv\Lib\site-packages\dateutil\parser\_parser.py:917:            s = tokens[idx]
venv\Lib\site-packages\dateutil\parser\_parser.py:929:        elif self._find_hms_idx(idx, tokens, info, allow_jump=True) is not None:
venv\Lib\site-packages\dateutil\parser\_parser.py:931:            hms_idx = self._find_hms_idx(idx, tokens, info, allow_jump=True)
venv\Lib\site-packages\dateutil\parser\_parser.py:932:            (idx, hms) = self._parse_hms(idx, tokens, info, hms_idx)
venv\Lib\site-packages\dateutil\parser\_parser.py:938:        elif idx + 2 < len_l and tokens[idx + 1] == ':':
venv\Lib\site-packages\dateutil\parser\_parser.py:941:            value = self._to_decimal(tokens[idx + 2])  # TODO: try/except for this?
venv\Lib\site-packages\dateutil\parser\_parser.py:944:            if idx + 4 < len_l and tokens[idx + 3] == ':':
venv\Lib\site-packages\dateutil\parser\_parser.py:945:                res.second, res.microsecond = self._parsems(tokens[idx + 4])
venv\Lib\site-packages\dateutil\parser\_parser.py:951:        elif idx + 1 < len_l and tokens[idx + 1] in ('-', '/', '.'):
venv\Lib\site-packages\dateutil\parser\_parser.py:952:            sep = tokens[idx + 1]
venv\Lib\site-packages\dateutil\parser\_parser.py:955:            if idx + 2 < len_l and not info.jump(tokens[idx + 2]):
venv\Lib\site-packages\dateutil\parser\_parser.py:956:                if tokens[idx + 2].isdigit():
venv\Lib\site-packages\dateutil\parser\_parser.py:958:                    ymd.append(tokens[idx + 2])
venv\Lib\site-packages\dateutil\parser\_parser.py:961:                    value = info.month(tokens[idx + 2])
venv\Lib\site-packages\dateutil\parser\_parser.py:968:                if idx + 3 < len_l and tokens[idx + 3] == sep:
venv\Lib\site-packages\dateutil\parser\_parser.py:970:                    value = info.month(tokens[idx + 4])
venv\Lib\site-packages\dateutil\parser\_parser.py:975:                        ymd.append(tokens[idx + 4])
venv\Lib\site-packages\dateutil\parser\_parser.py:981:        elif idx + 1 >= len_l or info.jump(tokens[idx + 1]):
venv\Lib\site-packages\dateutil\parser\_parser.py:982:            if idx + 2 < len_l and info.ampm(tokens[idx + 2]) is not None:
venv\Lib\site-packages\dateutil\parser\_parser.py:985:                res.hour = self._adjust_ampm(hour, info.ampm(tokens[idx + 2]))
venv\Lib\site-packages\dateutil\parser\_parser.py:992:        elif info.ampm(tokens[idx + 1]) is not None and (0 <= value < 24):
venv\Lib\site-packages\dateutil\parser\_parser.py:995:            res.hour = self._adjust_ampm(hour, info.ampm(tokens[idx + 1]))
venv\Lib\site-packages\dateutil\parser\_parser.py:1006:    def _find_hms_idx(self, idx, tokens, info, allow_jump):
venv\Lib\site-packages\dateutil\parser\_parser.py:1007:        len_l = len(tokens)
venv\Lib\site-packages\dateutil\parser\_parser.py:1009:        if idx+1 < len_l and info.hms(tokens[idx+1]) is not None:
venv\Lib\site-packages\dateutil\parser\_parser.py:1010:            # There is an "h", "m", or "s" label following this token.  We take
venv\Lib\site-packages\dateutil\parser\_parser.py:1011:            # assign the upcoming label to the current token.
venv\Lib\site-packages\dateutil\parser\_parser.py:1015:        elif (allow_jump and idx+2 < len_l and tokens[idx+1] == ' ' and
venv\Lib\site-packages\dateutil\parser\_parser.py:1016:              info.hms(tokens[idx+2]) is not None):
venv\Lib\site-packages\dateutil\parser\_parser.py:1021:        elif idx > 0 and info.hms(tokens[idx-1]) is not None:
venv\Lib\site-packages\dateutil\parser\_parser.py:1022:            # There is a "h", "m", or "s" preceding this token.  Since neither
venv\Lib\site-packages\dateutil\parser\_parser.py:1024:            # token, so we use the previous label.
venv\Lib\site-packages\dateutil\parser\_parser.py:1028:        elif (1 < idx == len_l-1 and tokens[idx-1] == ' ' and
venv\Lib\site-packages\dateutil\parser\_parser.py:1029:              info.hms(tokens[idx-2]) is not None):
venv\Lib\site-packages\dateutil\parser\_parser.py:1030:            # If we are looking at the final token, we allow for a
venv\Lib\site-packages\dateutil\parser\_parser.py:1056:    def _could_be_tzname(self, hour, tzname, tzoffset, token):
venv\Lib\site-packages\dateutil\parser\_parser.py:1060:                len(token) <= 5 and
venv\Lib\site-packages\dateutil\parser\_parser.py:1061:                (all(x in string.ascii_uppercase for x in token)
venv\Lib\site-packages\dateutil\parser\_parser.py:1062:                 or token in self.info.UTCZONE))
venv\Lib\site-packages\dateutil\parser\_parser.py:1111:    def _parse_hms(self, idx, tokens, info, hms_idx):
venv\Lib\site-packages\dateutil\parser\_parser.py:1120:            hms = info.hms(tokens[hms_idx])
venv\Lib\site-packages\dateutil\parser\_parser.py:1124:            hms = info.hms(tokens[hms_idx]) + 1
venv\Lib\site-packages\dateutil\parser\_parser.py:1130:    # Handling for individual tokens.  These are kept as methods instead
venv\Lib\site-packages\dateutil\parser\_parser.py:1250:    def _recombine_skipped(self, tokens, skipped_idxs):
venv\Lib\site-packages\dateutil\parser\_parser.py:1252:        >>> tokens = ["foo", " ", "bar", " ", "19June2000", "baz"]
venv\Lib\site-packages\dateutil\parser\_parser.py:1254:        >>> _recombine_skipped(tokens, skipped_idxs)
venv\Lib\site-packages\dateutil\parser\_parser.py:1257:        skipped_tokens = []
venv\Lib\site-packages\dateutil\parser\_parser.py:1260:                skipped_tokens[-1] = skipped_tokens[-1] + tokens[idx]
venv\Lib\site-packages\dateutil\parser\_parser.py:1262:                skipped_tokens.append(tokens[idx])
venv\Lib\site-packages\dateutil\parser\_parser.py:1264:        return skipped_tokens
venv\Lib\site-packages\dateutil\parser\_parser.py:1338:    :param fuzzy_with_tokens:
venv\Lib\site-packages\dateutil\parser\_parser.py:1347:            >>> parse("Today is January 1, 2047 at 8:21:00AM", fuzzy_with_tokens=True)
venv\Lib\site-packages\dateutil\parser\_parser.py:1352:        ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the
venv\Lib\site-packages\dateutil\parser\_parser.py:1354:        a tuple containing the fuzzy tokens.
venv\Lib\site-packages\dateutil\parser\_parser.py:1578:        res.any_unused_tokens = not {l[n] for n in unused_idxs}.issubset({",",":"})
venv\Lib\site-packages\dateutil\tz\tz.py:1086:        if res is None or res.any_unused_tokens:
venv\Lib\site-packages\Evtx\Evtx.py:400:                token = self.unpack_byte(ofs - 10)
venv\Lib\site-packages\Evtx\Evtx.py:402:                if token != 0x0C or pointer != ofs:
venv\Lib\site-packages\Evtx\Evtx.py:403:                    logger.warning("Unexpected token encountered")
venv\Lib\site-packages\Evtx\Nodes.py:29:class SYSTEM_TOKENS:
venv\Lib\site-packages\Evtx\Nodes.py:30:    EndOfStreamToken = 0x00
venv\Lib\site-packages\Evtx\Nodes.py:31:    OpenStartElementToken = 0x01
venv\Lib\site-packages\Evtx\Nodes.py:32:    CloseStartElementToken = 0x02
venv\Lib\site-packages\Evtx\Nodes.py:33:    CloseEmptyElementToken = 0x03
venv\Lib\site-packages\Evtx\Nodes.py:34:    CloseElementToken = 0x04
venv\Lib\site-packages\Evtx\Nodes.py:35:    ValueToken = 0x05
venv\Lib\site-packages\Evtx\Nodes.py:36:    AttributeToken = 0x06
venv\Lib\site-packages\Evtx\Nodes.py:37:    CDataSectionToken = 0x07
venv\Lib\site-packages\Evtx\Nodes.py:38:    EntityReferenceToken = 0x08
venv\Lib\site-packages\Evtx\Nodes.py:39:    ProcessingInstructionTargetToken = 0x0A
venv\Lib\site-packages\Evtx\Nodes.py:40:    ProcessingInstructionDataToken = 0x0B
venv\Lib\site-packages\Evtx\Nodes.py:41:    TemplateInstanceToken = 0x0C
venv\Lib\site-packages\Evtx\Nodes.py:42:    NormalSubstitutionToken = 0x0D
venv\Lib\site-packages\Evtx\Nodes.py:43:    ConditionalSubstitutionToken = 0x0E
venv\Lib\site-packages\Evtx\Nodes.py:44:    StartOfStreamToken = 0x0F
venv\Lib\site-packages\Evtx\Nodes.py:75:node_readable_tokens = []  # updated at end of file
venv\Lib\site-packages\Evtx\Nodes.py:133:    def _children(self, max_children=None, end_tokens=[SYSTEM_TOKENS.EndOfStreamToken]):
venv\Lib\site-packages\Evtx\Nodes.py:147:            #   but, some tokens like 0x01, make use of the flags nibble.
venv\Lib\site-packages\Evtx\Nodes.py:148:            token = self.unpack_byte(ofs) & 0x0F
venv\Lib\site-packages\Evtx\Nodes.py:150:                HandlerNodeClass = node_dispatch_table[token]
venv\Lib\site-packages\Evtx\Nodes.py:153:                raise ParseException("Unexpected token {:02X} at {}".format(token, self.absolute_offset(0x0) + ofs))
venv\Lib\site-packages\Evtx\Nodes.py:156:            if token in end_tokens:
venv\Lib\site-packages\Evtx\Nodes.py:240:    The binary XML node for the system token 0x00.
venv\Lib\site-packages\Evtx\Nodes.py:242:    This is the "end of stream" token. It may never actually
venv\Lib\site-packages\Evtx\Nodes.py:255:        return "EndOfStreamNode(offset={}, length={}, token={})".format(hex(self.offset()), hex(self.length()), 0x00)
venv\Lib\site-packages\Evtx\Nodes.py:258:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:272:    The binary XML node for the system token 0x01.
venv\Lib\site-packages\Evtx\Nodes.py:274:    This is the "open start element" token.
venv\Lib\site-packages\Evtx\Nodes.py:279:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:298:        return "OpenStartElementNode(offset={}, name={}, length={}, token={}, end={}, taglength={}, endtag={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:302:            hex(self.token()),
venv\Lib\site-packages\Evtx\Nodes.py:316:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:330:        return self._children(end_tokens=[SYSTEM_TOKENS.CloseElementToken, SYSTEM_TOKENS.CloseEmptyElementToken])
venv\Lib\site-packages\Evtx\Nodes.py:335:    The binary XML node for the system token 0x02.
venv\Lib\site-packages\Evtx\Nodes.py:337:    This is the "close start element" token.
venv\Lib\site-packages\Evtx\Nodes.py:342:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:350:        return "CloseStartElementNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:351:            hex(self.offset()), hex(self.length()), hex(self.token())
venv\Lib\site-packages\Evtx\Nodes.py:355:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:372:    The binary XML node for the system token 0x03.
venv\Lib\site-packages\Evtx\Nodes.py:377:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:385:        return "CloseEmptyElementNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:390:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:404:    The binary XML node for the system token 0x04.
venv\Lib\site-packages\Evtx\Nodes.py:406:    This is the "close element" token.
venv\Lib\site-packages\Evtx\Nodes.py:411:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:419:        return "CloseElementNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:420:            hex(self.offset()), hex(self.length()), hex(self.token())
venv\Lib\site-packages\Evtx\Nodes.py:424:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:479:    The binary XML node for the system token 0x05.
venv\Lib\site-packages\Evtx\Nodes.py:481:    This is the "value" token.
venv\Lib\site-packages\Evtx\Nodes.py:486:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:495:        return "ValueNode(offset={}, length={}, token={}, value={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:496:            hex(self.offset()), hex(self.length()), hex(self.token()), self.value().string()
venv\Lib\site-packages\Evtx\Nodes.py:500:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:513:        return self.flags() & 0x0B == 0 and self.token() & 0x0F == SYSTEM_TOKENS.ValueToken
venv\Lib\site-packages\Evtx\Nodes.py:518:    The binary XML node for the system token 0x06.
venv\Lib\site-packages\Evtx\Nodes.py:520:    This is the "attribute" token.
venv\Lib\site-packages\Evtx\Nodes.py:525:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:539:        return "AttributeNode(offset={}, length={}, token={}, name={}, value={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:540:            hex(self.offset()), hex(self.length()), hex(self.token()), self.attribute_name(), self.attribute_value()
venv\Lib\site-packages\Evtx\Nodes.py:544:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:572:    The binary XML node for the system token 0x07.
venv\Lib\site-packages\Evtx\Nodes.py:574:    This is the "CDATA section" system token.
venv\Lib\site-packages\Evtx\Nodes.py:579:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:589:        return "CDataSectionNode(offset={}, length={}, token={})".format(hex(self.offset()), hex(self.length()), 0x07)
venv\Lib\site-packages\Evtx\Nodes.py:592:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:604:        return self.flags() == 0x0 and self.token() & 0x0F == SYSTEM_TOKENS.CDataSectionToken
venv\Lib\site-packages\Evtx\Nodes.py:609:    The binary XML node for the system token 0x08.
venv\Lib\site-packages\Evtx\Nodes.py:617:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:627:        return "CharacterReferenceNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:635:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:646:    The binary XML node for the system token 0x09.
venv\Lib\site-packages\Evtx\Nodes.py:656:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:670:        return "EntityReferenceNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:678:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:690:    The binary XML node for the system token 0x0A.
venv\Lib\site-packages\Evtx\Nodes.py:697:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:711:        return "ProcessingInstructionTargetNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:719:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:731:    The binary XML node for the system token 0x0B.
venv\Lib\site-packages\Evtx\Nodes.py:738:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:753:        return "ProcessingInstructionDataNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:758:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:776:    The binary XML node for the system token 0x0C.
venv\Lib\site-packages\Evtx\Nodes.py:781:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:798:        return "TemplateInstanceNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:803:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:827:    The binary XML node for the system token 0x0D.
venv\Lib\site-packages\Evtx\Nodes.py:829:    This is a "normal substitution" token.
venv\Lib\site-packages\Evtx\Nodes.py:834:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:844:        return "NormalSubstitutionNode(offset={}, length={}, token={}, index={}, type={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:845:            hex(self.offset()), hex(self.length()), hex(self.token()), self.index(), self.type()
venv\Lib\site-packages\Evtx\Nodes.py:849:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:861:        return self.flags() == 0 and self.token() & 0x0F == SYSTEM_TOKENS.NormalSubstitutionToken
venv\Lib\site-packages\Evtx\Nodes.py:866:    The binary XML node for the system token 0x0E.
venv\Lib\site-packages\Evtx\Nodes.py:871:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:881:        return "ConditionalSubstitutionNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:890:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:902:        return self.flags() == 0 and self.token() & 0x0F == SYSTEM_TOKENS.ConditionalSubstitutionToken
venv\Lib\site-packages\Evtx\Nodes.py:907:    The binary XML node for the system token 0x0F.
venv\Lib\site-packages\Evtx\Nodes.py:909:    This is the "start of stream" token.
venv\Lib\site-packages\Evtx\Nodes.py:914:        self.declare_field("byte", "token", 0x0)
venv\Lib\site-packages\Evtx\Nodes.py:924:        return "StreamStartNode(offset={}, length={}, token={})".format(
venv\Lib\site-packages\Evtx\Nodes.py:925:            hex(self.offset()), hex(self.length()), hex(self.token())
venv\Lib\site-packages\Evtx\Nodes.py:931:            and self.token() & 0x0F == SYSTEM_TOKENS.StartOfStreamToken
venv\Lib\site-packages\Evtx\Nodes.py:937:        return self.token() >> 4
venv\Lib\site-packages\Evtx\Nodes.py:973:        return self._children(end_tokens=[SYSTEM_TOKENS.EndOfStreamToken])
venv\Lib\site-packages\Evtx\Nodes.py:1588:node_readable_tokens = [
venv\Lib\site-packages\flask\app.py:183:            "SECRET_KEY": None,
venv\Lib\site-packages\flask\app.py:184:            "SECRET_KEY_FALLBACKS": None,
venv\Lib\site-packages\flask\config.py:66:        SECRET_KEY = 'development key'
venv\Lib\site-packages\flask\ctx.py:249:        self._cv_tokens: list[contextvars.Token[AppContext]] = []
venv\Lib\site-packages\flask\ctx.py:253:        self._cv_tokens.append(_cv_app.set(self))
venv\Lib\site-packages\flask\ctx.py:259:            if len(self._cv_tokens) == 1:
venv\Lib\site-packages\flask\ctx.py:265:            _cv_app.reset(self._cv_tokens.pop())
venv\Lib\site-packages\flask\ctx.py:333:        self._cv_tokens: list[
venv\Lib\site-packages\flask\ctx.py:334:            tuple[contextvars.Token[RequestContext], AppContext | None]
venv\Lib\site-packages\flask\ctx.py:378:        self._cv_tokens.append((_cv_request.set(self), app_ctx))
venv\Lib\site-packages\flask\ctx.py:404:        clear_request = len(self._cv_tokens) == 1
venv\Lib\site-packages\flask\ctx.py:417:            token, app_ctx = self._cv_tokens.pop()
venv\Lib\site-packages\flask\ctx.py:418:            _cv_request.reset(token)
venv\Lib\site-packages\flask\sansio\app.py:210:    #: If a secret key is set, cryptographic components can use this to
venv\Lib\site-packages\flask\sansio\app.py:215:    #: :data:`SECRET_KEY` configuration key. Defaults to ``None``.
venv\Lib\site-packages\flask\sansio\app.py:216:    secret_key = ConfigAttribute[t.Union[str, bytes, None]]("SECRET_KEY")
venv\Lib\site-packages\flask\sessions.py:105:            "The session is unavailable because no secret "
venv\Lib\site-packages\flask\sessions.py:106:            "key was set.  Set the secret_key on the "
venv\Lib\site-packages\flask\sessions.py:107:            "application to something unique and secret."
venv\Lib\site-packages\flask\sessions.py:133:    will complain that the secret key was not set.
venv\Lib\site-packages\flask\sessions.py:303:    #: the salt that should be applied on top of the secret key for the
venv\Lib\site-packages\flask\sessions.py:318:        if not app.secret_key:
venv\Lib\site-packages\flask\sessions.py:323:        if fallbacks := app.config["SECRET_KEY_FALLBACKS"]:
venv\Lib\site-packages\flask\sessions.py:326:        keys.append(app.secret_key)  # itsdangerous expects current key at top
venv\Lib\site-packages\itsdangerous\serializer.py:51:    The secret key should be a random string of ``bytes`` and should not
venv\Lib\site-packages\itsdangerous\serializer.py:54:    for information about the security of the secret key and salt.
venv\Lib\site-packages\itsdangerous\serializer.py:56:    :param secret_key: The secret key to sign and verify with. Can be a
venv\Lib\site-packages\itsdangerous\serializer.py:58:    :param salt: Extra key to combine with ``secret_key`` to distinguish
venv\Lib\site-packages\itsdangerous\serializer.py:78:        ``secret_key``.
venv\Lib\site-packages\itsdangerous\serializer.py:112:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\serializer.py:128:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\serializer.py:144:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\serializer.py:163:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\serializer.py:179:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\serializer.py:194:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\serializer.py:205:        #: The list of secret keys to try for verifying signatures, from
venv\Lib\site-packages\itsdangerous\serializer.py:210:        self.secret_keys: list[bytes] = _make_keys_list(secret_key)
venv\Lib\site-packages\itsdangerous\serializer.py:239:    def secret_key(self) -> bytes:
venv\Lib\site-packages\itsdangerous\serializer.py:240:        """The newest (last) entry in the :attr:`secret_keys` list. This
venv\Lib\site-packages\itsdangerous\serializer.py:243:        return self.secret_keys[-1]
venv\Lib\site-packages\itsdangerous\serializer.py:287:        return self.signer(self.secret_keys, salt=salt, **self.signer_kwargs)
venv\Lib\site-packages\itsdangerous\serializer.py:308:            for secret_key in self.secret_keys:
venv\Lib\site-packages\itsdangerous\serializer.py:309:                yield fallback(secret_key, salt=salt, **kwargs)
venv\Lib\site-packages\itsdangerous\signer.py:68:    secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\signer.py:70:    if isinstance(secret_key, (str, bytes)):
venv\Lib\site-packages\itsdangerous\signer.py:71:        return [want_bytes(secret_key)]
venv\Lib\site-packages\itsdangerous\signer.py:73:    return [want_bytes(s) for s in secret_key]  # pyright: ignore
venv\Lib\site-packages\itsdangerous\signer.py:80:    The secret key should be a random string of ``bytes`` and should not
venv\Lib\site-packages\itsdangerous\signer.py:83:    for information about the security of the secret key and salt.
venv\Lib\site-packages\itsdangerous\signer.py:85:    :param secret_key: The secret key to sign and verify with. Can be a
venv\Lib\site-packages\itsdangerous\signer.py:87:    :param salt: Extra key to combine with ``secret_key`` to distinguish
venv\Lib\site-packages\itsdangerous\signer.py:90:    :param key_derivation: How to derive the signing key from the secret
venv\Lib\site-packages\itsdangerous\signer.py:104:        ``secret_key``.
venv\Lib\site-packages\itsdangerous\signer.py:123:    #: secret key and salt. The default is ``django-concat``. Possible
venv\Lib\site-packages\itsdangerous\signer.py:131:        secret_key: str | bytes | cabc.Iterable[str] | cabc.Iterable[bytes],
venv\Lib\site-packages\itsdangerous\signer.py:138:        #: The list of secret keys to try for verifying signatures, from
venv\Lib\site-packages\itsdangerous\signer.py:143:        self.secret_keys: list[bytes] = _make_keys_list(secret_key)
venv\Lib\site-packages\itsdangerous\signer.py:176:    def secret_key(self) -> bytes:
venv\Lib\site-packages\itsdangerous\signer.py:177:        """The newest (last) entry in the :attr:`secret_keys` list. This
venv\Lib\site-packages\itsdangerous\signer.py:180:        return self.secret_keys[-1]
venv\Lib\site-packages\itsdangerous\signer.py:182:    def derive_key(self, secret_key: str | bytes | None = None) -> bytes:
venv\Lib\site-packages\itsdangerous\signer.py:186:        out of a short password. Instead you should use large random
venv\Lib\site-packages\itsdangerous\signer.py:187:        secret keys.
venv\Lib\site-packages\itsdangerous\signer.py:189:        :param secret_key: A specific secret key to derive from.
venv\Lib\site-packages\itsdangerous\signer.py:190:            Defaults to the last item in :attr:`secret_keys`.
venv\Lib\site-packages\itsdangerous\signer.py:193:            Added the ``secret_key`` parameter.
venv\Lib\site-packages\itsdangerous\signer.py:195:        if secret_key is None:
venv\Lib\site-packages\itsdangerous\signer.py:196:            secret_key = self.secret_keys[-1]
venv\Lib\site-packages\itsdangerous\signer.py:198:            secret_key = want_bytes(secret_key)
venv\Lib\site-packages\itsdangerous\signer.py:201:            return t.cast(bytes, self.digest_method(self.salt + secret_key).digest())
venv\Lib\site-packages\itsdangerous\signer.py:204:                bytes, self.digest_method(self.salt + b"signer" + secret_key).digest()
venv\Lib\site-packages\itsdangerous\signer.py:207:            mac = hmac.new(secret_key, digestmod=self.digest_method)
venv\Lib\site-packages\itsdangerous\signer.py:211:            return secret_key
venv\Lib\site-packages\itsdangerous\signer.py:236:        for secret_key in reversed(self.secret_keys):
venv\Lib\site-packages\itsdangerous\signer.py:237:            key = self.derive_key(secret_key)
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:26:token has not been tampered with.
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:30:loading a token.
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:35:Here's how you could generate a token for transmitting a user's id and
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:40:auth_s = URLSafeSerializer("secret key", "auth")
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:41:token = auth_s.dumps({"id": 5, "name": "itsdangerous"})
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:43:print(token)
venv\Lib\site-packages\itsdangerous-2.2.0.dist-info\METADATA:46:data = auth_s.loads(token)
venv\Lib\site-packages\jinja2\environment.py:43:from .lexer import TokenStream
venv\Lib\site-packages\jinja2\environment.py:631:        tokens as tuples in the form ``(lineno, token_type, value)``.
venv\Lib\site-packages\jinja2\environment.py:641:            return self.lexer.tokeniter(source, name, filename)
venv\Lib\site-packages\jinja2\environment.py:653:        because there you usually only want the actual source tokenized.
venv\Lib\site-packages\jinja2\environment.py:661:    def _tokenize(
venv\Lib\site-packages\jinja2\environment.py:667:    ) -> TokenStream:
venv\Lib\site-packages\jinja2\environment.py:669:        for all the extensions.  Returns a :class:`~jinja2.lexer.TokenStream`.
venv\Lib\site-packages\jinja2\environment.py:672:        stream = self.lexer.tokenize(source, name, filename, state)
venv\Lib\site-packages\jinja2\environment.py:677:            if not isinstance(stream, TokenStream):
venv\Lib\site-packages\jinja2\environment.py:678:                stream = TokenStream(stream, name, filename)
venv\Lib\site-packages\jinja2\ext.py:23:    from .lexer import Token
venv\Lib\site-packages\jinja2\ext.py:24:    from .lexer import TokenStream
venv\Lib\site-packages\jinja2\ext.py:109:        self, stream: "TokenStream"
venv\Lib\site-packages\jinja2\ext.py:110:    ) -> t.Union["TokenStream", t.Iterable["Token"]]:
venv\Lib\site-packages\jinja2\ext.py:111:        """It's passed a :class:`~jinja2.lexer.TokenStream` that can be used
venv\Lib\site-packages\jinja2\ext.py:112:        to filter tokens returned.  This method has to return an iterable of
venv\Lib\site-packages\jinja2\ext.py:113:        :class:`~jinja2.lexer.Token`\\s, but it doesn't have to return a
venv\Lib\site-packages\jinja2\ext.py:114:        :class:`~jinja2.lexer.TokenStream`.
venv\Lib\site-packages\jinja2\ext.py:120:        parser as first argument.  The token the parser stream is pointing at
venv\Lib\site-packages\jinja2\ext.py:121:        is the name token that matched.  This method has to return one or a
venv\Lib\site-packages\jinja2\ext.py:360:        context_token = parser.stream.next_if("string")
venv\Lib\site-packages\jinja2\ext.py:362:        if context_token is not None:
venv\Lib\site-packages\jinja2\ext.py:363:            context = context_token.value
venv\Lib\site-packages\jinja2\ext.py:381:            token = parser.stream.expect("name")
venv\Lib\site-packages\jinja2\ext.py:382:            if token.value in variables:
venv\Lib\site-packages\jinja2\ext.py:384:                    f"translatable variable {token.value!r} defined twice.",
venv\Lib\site-packages\jinja2\ext.py:385:                    token.lineno,
venv\Lib\site-packages\jinja2\ext.py:392:                variables[token.value] = var = parser.parse_expression()
venv\Lib\site-packages\jinja2\ext.py:393:            elif trimmed is None and token.value in ("trimmed", "notrimmed"):
venv\Lib\site-packages\jinja2\ext.py:394:                trimmed = token.value == "trimmed"
venv\Lib\site-packages\jinja2\ext.py:397:                variables[token.value] = var = nodes.Name(token.value, "load")
venv\Lib\site-packages\jinja2\ext.py:402:                    variables[token.value] = plural_expr
venv\Lib\site-packages\jinja2\ext.py:408:                num_called_num = token.value == "num"
venv\Lib\site-packages\jinja2\ext.py:429:                token = parser.stream.expect("name")
venv\Lib\site-packages\jinja2\ext.py:430:                if token.value not in variables:
venv\Lib\site-packages\jinja2\ext.py:432:                        f"unknown variable {token.value!r} for pluralization",
venv\Lib\site-packages\jinja2\ext.py:433:                        token.lineno,
venv\Lib\site-packages\jinja2\ext.py:436:                plural_expr = variables[token.value]
venv\Lib\site-packages\jinja2\ext.py:437:                num_called_num = token.value == "num"
venv\Lib\site-packages\jinja2\ext.py:609:        token = next(parser.stream)
venv\Lib\site-packages\jinja2\ext.py:610:        if token.value == "break":
venv\Lib\site-packages\jinja2\ext.py:611:            return nodes.Break(lineno=token.lineno)
venv\Lib\site-packages\jinja2\ext.py:612:        return nodes.Continue(lineno=token.lineno)
venv\Lib\site-packages\jinja2\ext.py:735:    """Helper class to find comments in a token stream.  Can only
venv\Lib\site-packages\jinja2\ext.py:742:        self, tokens: t.Sequence[t.Tuple[int, str, str]], comment_tags: t.Sequence[str]
venv\Lib\site-packages\jinja2\ext.py:744:        self.tokens = tokens
venv\Lib\site-packages\jinja2\ext.py:751:            for _, token_type, token_value in reversed(
venv\Lib\site-packages\jinja2\ext.py:752:                self.tokens[self.offset : offset]
venv\Lib\site-packages\jinja2\ext.py:754:                if token_type in ("comment", "linecomment"):
venv\Lib\site-packages\jinja2\ext.py:756:                        prefix, comment = token_value.split(None, 1)
venv\Lib\site-packages\jinja2\ext.py:768:        for idx, (token_lineno, _, _) in enumerate(self.tokens[self.offset :]):
venv\Lib\site-packages\jinja2\ext.py:769:            if token_lineno > lineno:
venv\Lib\site-packages\jinja2\ext.py:771:        return self.find_backwards(len(self.tokens))
venv\Lib\site-packages\jinja2\ext.py:854:        tokens = list(environment.lex(environment.preprocess(source)))
venv\Lib\site-packages\jinja2\ext.py:861:    finder = _CommentFinder(tokens, comment_tags)
venv\Lib\site-packages\jinja2\lexer.py:62:# internal the tokens and keep references to them
venv\Lib\site-packages\jinja2\lexer.py:63:TOKEN_ADD = intern("add")
venv\Lib\site-packages\jinja2\lexer.py:64:TOKEN_ASSIGN = intern("assign")
venv\Lib\site-packages\jinja2\lexer.py:65:TOKEN_COLON = intern("colon")
venv\Lib\site-packages\jinja2\lexer.py:66:TOKEN_COMMA = intern("comma")
venv\Lib\site-packages\jinja2\lexer.py:67:TOKEN_DIV = intern("div")
venv\Lib\site-packages\jinja2\lexer.py:68:TOKEN_DOT = intern("dot")
venv\Lib\site-packages\jinja2\lexer.py:69:TOKEN_EQ = intern("eq")
venv\Lib\site-packages\jinja2\lexer.py:70:TOKEN_FLOORDIV = intern("floordiv")
venv\Lib\site-packages\jinja2\lexer.py:71:TOKEN_GT = intern("gt")
venv\Lib\site-packages\jinja2\lexer.py:72:TOKEN_GTEQ = intern("gteq")
venv\Lib\site-packages\jinja2\lexer.py:73:TOKEN_LBRACE = intern("lbrace")
venv\Lib\site-packages\jinja2\lexer.py:74:TOKEN_LBRACKET = intern("lbracket")
venv\Lib\site-packages\jinja2\lexer.py:75:TOKEN_LPAREN = intern("lparen")
venv\Lib\site-packages\jinja2\lexer.py:76:TOKEN_LT = intern("lt")
venv\Lib\site-packages\jinja2\lexer.py:77:TOKEN_LTEQ = intern("lteq")
venv\Lib\site-packages\jinja2\lexer.py:78:TOKEN_MOD = intern("mod")
venv\Lib\site-packages\jinja2\lexer.py:79:TOKEN_MUL = intern("mul")
venv\Lib\site-packages\jinja2\lexer.py:80:TOKEN_NE = intern("ne")
venv\Lib\site-packages\jinja2\lexer.py:81:TOKEN_PIPE = intern("pipe")
venv\Lib\site-packages\jinja2\lexer.py:82:TOKEN_POW = intern("pow")
venv\Lib\site-packages\jinja2\lexer.py:83:TOKEN_RBRACE = intern("rbrace")
venv\Lib\site-packages\jinja2\lexer.py:84:TOKEN_RBRACKET = intern("rbracket")
venv\Lib\site-packages\jinja2\lexer.py:85:TOKEN_RPAREN = intern("rparen")
venv\Lib\site-packages\jinja2\lexer.py:86:TOKEN_SEMICOLON = intern("semicolon")
venv\Lib\site-packages\jinja2\lexer.py:87:TOKEN_SUB = intern("sub")
venv\Lib\site-packages\jinja2\lexer.py:88:TOKEN_TILDE = intern("tilde")
venv\Lib\site-packages\jinja2\lexer.py:89:TOKEN_WHITESPACE = intern("whitespace")
venv\Lib\site-packages\jinja2\lexer.py:90:TOKEN_FLOAT = intern("float")
venv\Lib\site-packages\jinja2\lexer.py:91:TOKEN_INTEGER = intern("integer")
venv\Lib\site-packages\jinja2\lexer.py:92:TOKEN_NAME = intern("name")
venv\Lib\site-packages\jinja2\lexer.py:93:TOKEN_STRING = intern("string")
venv\Lib\site-packages\jinja2\lexer.py:94:TOKEN_OPERATOR = intern("operator")
venv\Lib\site-packages\jinja2\lexer.py:95:TOKEN_BLOCK_BEGIN = intern("block_begin")
venv\Lib\site-packages\jinja2\lexer.py:96:TOKEN_BLOCK_END = intern("block_end")
venv\Lib\site-packages\jinja2\lexer.py:97:TOKEN_VARIABLE_BEGIN = intern("variable_begin")
venv\Lib\site-packages\jinja2\lexer.py:98:TOKEN_VARIABLE_END = intern("variable_end")
venv\Lib\site-packages\jinja2\lexer.py:99:TOKEN_RAW_BEGIN = intern("raw_begin")
venv\Lib\site-packages\jinja2\lexer.py:100:TOKEN_RAW_END = intern("raw_end")
venv\Lib\site-packages\jinja2\lexer.py:101:TOKEN_COMMENT_BEGIN = intern("comment_begin")
venv\Lib\site-packages\jinja2\lexer.py:102:TOKEN_COMMENT_END = intern("comment_end")
venv\Lib\site-packages\jinja2\lexer.py:103:TOKEN_COMMENT = intern("comment")
venv\Lib\site-packages\jinja2\lexer.py:104:TOKEN_LINESTATEMENT_BEGIN = intern("linestatement_begin")
venv\Lib\site-packages\jinja2\lexer.py:105:TOKEN_LINESTATEMENT_END = intern("linestatement_end")
venv\Lib\site-packages\jinja2\lexer.py:106:TOKEN_LINECOMMENT_BEGIN = intern("linecomment_begin")
venv\Lib\site-packages\jinja2\lexer.py:107:TOKEN_LINECOMMENT_END = intern("linecomment_end")
venv\Lib\site-packages\jinja2\lexer.py:108:TOKEN_LINECOMMENT = intern("linecomment")
venv\Lib\site-packages\jinja2\lexer.py:109:TOKEN_DATA = intern("data")
venv\Lib\site-packages\jinja2\lexer.py:110:TOKEN_INITIAL = intern("initial")
venv\Lib\site-packages\jinja2\lexer.py:111:TOKEN_EOF = intern("eof")
venv\Lib\site-packages\jinja2\lexer.py:113:# bind operators to token types
venv\Lib\site-packages\jinja2\lexer.py:115:    "+": TOKEN_ADD,
venv\Lib\site-packages\jinja2\lexer.py:116:    "-": TOKEN_SUB,
venv\Lib\site-packages\jinja2\lexer.py:117:    "/": TOKEN_DIV,
venv\Lib\site-packages\jinja2\lexer.py:118:    "//": TOKEN_FLOORDIV,
venv\Lib\site-packages\jinja2\lexer.py:119:    "*": TOKEN_MUL,
venv\Lib\site-packages\jinja2\lexer.py:120:    "%": TOKEN_MOD,
venv\Lib\site-packages\jinja2\lexer.py:121:    "**": TOKEN_POW,
venv\Lib\site-packages\jinja2\lexer.py:122:    "~": TOKEN_TILDE,
venv\Lib\site-packages\jinja2\lexer.py:123:    "[": TOKEN_LBRACKET,
venv\Lib\site-packages\jinja2\lexer.py:124:    "]": TOKEN_RBRACKET,
venv\Lib\site-packages\jinja2\lexer.py:125:    "(": TOKEN_LPAREN,
venv\Lib\site-packages\jinja2\lexer.py:126:    ")": TOKEN_RPAREN,
venv\Lib\site-packages\jinja2\lexer.py:127:    "{": TOKEN_LBRACE,
venv\Lib\site-packages\jinja2\lexer.py:128:    "}": TOKEN_RBRACE,
venv\Lib\site-packages\jinja2\lexer.py:129:    "==": TOKEN_EQ,
venv\Lib\site-packages\jinja2\lexer.py:130:    "!=": TOKEN_NE,
venv\Lib\site-packages\jinja2\lexer.py:131:    ">": TOKEN_GT,
venv\Lib\site-packages\jinja2\lexer.py:132:    ">=": TOKEN_GTEQ,
venv\Lib\site-packages\jinja2\lexer.py:133:    "<": TOKEN_LT,
venv\Lib\site-packages\jinja2\lexer.py:134:    "<=": TOKEN_LTEQ,
venv\Lib\site-packages\jinja2\lexer.py:135:    "=": TOKEN_ASSIGN,
venv\Lib\site-packages\jinja2\lexer.py:136:    ".": TOKEN_DOT,
venv\Lib\site-packages\jinja2\lexer.py:137:    ":": TOKEN_COLON,
venv\Lib\site-packages\jinja2\lexer.py:138:    "|": TOKEN_PIPE,
venv\Lib\site-packages\jinja2\lexer.py:139:    ",": TOKEN_COMMA,
venv\Lib\site-packages\jinja2\lexer.py:140:    ";": TOKEN_SEMICOLON,
venv\Lib\site-packages\jinja2\lexer.py:149:ignored_tokens = frozenset(
venv\Lib\site-packages\jinja2\lexer.py:151:        TOKEN_COMMENT_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:152:        TOKEN_COMMENT,
venv\Lib\site-packages\jinja2\lexer.py:153:        TOKEN_COMMENT_END,
venv\Lib\site-packages\jinja2\lexer.py:154:        TOKEN_WHITESPACE,
venv\Lib\site-packages\jinja2\lexer.py:155:        TOKEN_LINECOMMENT_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:156:        TOKEN_LINECOMMENT_END,
venv\Lib\site-packages\jinja2\lexer.py:157:        TOKEN_LINECOMMENT,
venv\Lib\site-packages\jinja2\lexer.py:161:    [TOKEN_WHITESPACE, TOKEN_DATA, TOKEN_COMMENT, TOKEN_LINECOMMENT]
venv\Lib\site-packages\jinja2\lexer.py:165:def _describe_token_type(token_type: str) -> str:
venv\Lib\site-packages\jinja2\lexer.py:166:    if token_type in reverse_operators:
venv\Lib\site-packages\jinja2\lexer.py:167:        return reverse_operators[token_type]
venv\Lib\site-packages\jinja2\lexer.py:170:        TOKEN_COMMENT_BEGIN: "begin of comment",
venv\Lib\site-packages\jinja2\lexer.py:171:        TOKEN_COMMENT_END: "end of comment",
venv\Lib\site-packages\jinja2\lexer.py:172:        TOKEN_COMMENT: "comment",
venv\Lib\site-packages\jinja2\lexer.py:173:        TOKEN_LINECOMMENT: "comment",
venv\Lib\site-packages\jinja2\lexer.py:174:        TOKEN_BLOCK_BEGIN: "begin of statement block",
venv\Lib\site-packages\jinja2\lexer.py:175:        TOKEN_BLOCK_END: "end of statement block",
venv\Lib\site-packages\jinja2\lexer.py:176:        TOKEN_VARIABLE_BEGIN: "begin of print statement",
venv\Lib\site-packages\jinja2\lexer.py:177:        TOKEN_VARIABLE_END: "end of print statement",
venv\Lib\site-packages\jinja2\lexer.py:178:        TOKEN_LINESTATEMENT_BEGIN: "begin of line statement",
venv\Lib\site-packages\jinja2\lexer.py:179:        TOKEN_LINESTATEMENT_END: "end of line statement",
venv\Lib\site-packages\jinja2\lexer.py:180:        TOKEN_DATA: "template data / text",
venv\Lib\site-packages\jinja2\lexer.py:181:        TOKEN_EOF: "end of template",
venv\Lib\site-packages\jinja2\lexer.py:182:    }.get(token_type, token_type)
venv\Lib\site-packages\jinja2\lexer.py:185:def describe_token(token: "Token") -> str:
venv\Lib\site-packages\jinja2\lexer.py:186:    """Returns a description of the token."""
venv\Lib\site-packages\jinja2\lexer.py:187:    if token.type == TOKEN_NAME:
venv\Lib\site-packages\jinja2\lexer.py:188:        return token.value
venv\Lib\site-packages\jinja2\lexer.py:190:    return _describe_token_type(token.type)
venv\Lib\site-packages\jinja2\lexer.py:193:def describe_token_expr(expr: str) -> str:
venv\Lib\site-packages\jinja2\lexer.py:194:    """Like `describe_token` but for token expressions."""
venv\Lib\site-packages\jinja2\lexer.py:198:        if type == TOKEN_NAME:
venv\Lib\site-packages\jinja2\lexer.py:203:    return _describe_token_type(type)
venv\Lib\site-packages\jinja2\lexer.py:219:            TOKEN_COMMENT_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:224:            TOKEN_BLOCK_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:229:            TOKEN_VARIABLE_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:238:                TOKEN_LINESTATEMENT_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:246:                TOKEN_LINECOMMENT_BEGIN,
venv\Lib\site-packages\jinja2\lexer.py:269:class Token(t.NamedTuple):
venv\Lib\site-packages\jinja2\lexer.py:275:        return describe_token(self)
venv\Lib\site-packages\jinja2\lexer.py:278:        """Test a token against a token expression.  This can either be a
venv\Lib\site-packages\jinja2\lexer.py:279:        token type or ``'token_type:token_value'``.  This can only test
venv\Lib\site-packages\jinja2\lexer.py:293:        """Test against multiple token expressions."""
venv\Lib\site-packages\jinja2\lexer.py:297:class TokenStreamIterator:
venv\Lib\site-packages\jinja2\lexer.py:298:    """The iterator for tokenstreams.  Iterate over the stream
venv\Lib\site-packages\jinja2\lexer.py:299:    until the eof token is reached.
venv\Lib\site-packages\jinja2\lexer.py:302:    def __init__(self, stream: "TokenStream") -> None:
venv\Lib\site-packages\jinja2\lexer.py:305:    def __iter__(self) -> "TokenStreamIterator":
venv\Lib\site-packages\jinja2\lexer.py:308:    def __next__(self) -> Token:
venv\Lib\site-packages\jinja2\lexer.py:309:        token = self.stream.current
venv\Lib\site-packages\jinja2\lexer.py:311:        if token.type is TOKEN_EOF:
venv\Lib\site-packages\jinja2\lexer.py:316:        return token
venv\Lib\site-packages\jinja2\lexer.py:319:class TokenStream:
venv\Lib\site-packages\jinja2\lexer.py:320:    """A token stream is an iterable that yields :class:`Token`\\s.  The
venv\Lib\site-packages\jinja2\lexer.py:322:    one token ahead.  The current active token is stored as :attr:`current`.
venv\Lib\site-packages\jinja2\lexer.py:327:        generator: t.Iterable[Token],
venv\Lib\site-packages\jinja2\lexer.py:332:        self._pushed: te.Deque[Token] = deque()
venv\Lib\site-packages\jinja2\lexer.py:336:        self.current = Token(1, TOKEN_INITIAL, "")
venv\Lib\site-packages\jinja2\lexer.py:339:    def __iter__(self) -> TokenStreamIterator:
venv\Lib\site-packages\jinja2\lexer.py:340:        return TokenStreamIterator(self)
venv\Lib\site-packages\jinja2\lexer.py:343:        return bool(self._pushed) or self.current.type is not TOKEN_EOF
venv\Lib\site-packages\jinja2\lexer.py:350:    def push(self, token: Token) -> None:
venv\Lib\site-packages\jinja2\lexer.py:351:        """Push a token back to the stream."""
venv\Lib\site-packages\jinja2\lexer.py:352:        self._pushed.append(token)
venv\Lib\site-packages\jinja2\lexer.py:354:    def look(self) -> Token:
venv\Lib\site-packages\jinja2\lexer.py:355:        """Look at the next token."""
venv\Lib\site-packages\jinja2\lexer.py:356:        old_token = next(self)
venv\Lib\site-packages\jinja2\lexer.py:359:        self.current = old_token
venv\Lib\site-packages\jinja2\lexer.py:363:        """Got n tokens ahead."""
venv\Lib\site-packages\jinja2\lexer.py:367:    def next_if(self, expr: str) -> t.Optional[Token]:
venv\Lib\site-packages\jinja2\lexer.py:368:        """Perform the token test and return the token if it matched.
venv\Lib\site-packages\jinja2\lexer.py:380:    def __next__(self) -> Token:
venv\Lib\site-packages\jinja2\lexer.py:381:        """Go one token ahead and return the old one.
venv\Lib\site-packages\jinja2\lexer.py:389:        elif self.current.type is not TOKEN_EOF:
venv\Lib\site-packages\jinja2\lexer.py:399:        self.current = Token(self.current.lineno, TOKEN_EOF, "")
venv\Lib\site-packages\jinja2\lexer.py:403:    def expect(self, expr: str) -> Token:
venv\Lib\site-packages\jinja2\lexer.py:404:        """Expect a given token type and return it.  This accepts the same
venv\Lib\site-packages\jinja2\lexer.py:405:        argument as :meth:`jinja2.lexer.Token.test`.
venv\Lib\site-packages\jinja2\lexer.py:408:            expr = describe_token_expr(expr)
venv\Lib\site-packages\jinja2\lexer.py:410:            if self.current.type is TOKEN_EOF:
venv\Lib\site-packages\jinja2\lexer.py:419:                f"expected token {expr!r}, got {describe_token(self.current)!r}",
venv\Lib\site-packages\jinja2\lexer.py:467:    tokens: t.Union[str, t.Tuple[str, ...], t.Tuple[Failure]]
venv\Lib\site-packages\jinja2\lexer.py:488:            _Rule(whitespace_re, TOKEN_WHITESPACE, None),
venv\Lib\site-packages\jinja2\lexer.py:489:            _Rule(float_re, TOKEN_FLOAT, None),
venv\Lib\site-packages\jinja2\lexer.py:490:            _Rule(integer_re, TOKEN_INTEGER, None),
venv\Lib\site-packages\jinja2\lexer.py:491:            _Rule(name_re, TOKEN_NAME, None),
venv\Lib\site-packages\jinja2\lexer.py:492:            _Rule(string_re, TOKEN_STRING, None),
venv\Lib\site-packages\jinja2\lexer.py:493:            _Rule(operator_re, TOKEN_OPERATOR, None),
venv\Lib\site-packages\jinja2\lexer.py:531:                    OptionalLStrip(TOKEN_DATA, "#bygroup"),  # type: ignore
venv\Lib\site-packages\jinja2\lexer.py:535:                _Rule(c(".+"), TOKEN_DATA, None),
venv\Lib\site-packages\jinja2\lexer.py:538:            TOKEN_COMMENT_BEGIN: [
venv\Lib\site-packages\jinja2\lexer.py:544:                    (TOKEN_COMMENT, TOKEN_COMMENT_END),
venv\Lib\site-packages\jinja2\lexer.py:550:            TOKEN_BLOCK_BEGIN: [
venv\Lib\site-packages\jinja2\lexer.py:556:                    TOKEN_BLOCK_END,
venv\Lib\site-packages\jinja2\lexer.py:562:            TOKEN_VARIABLE_BEGIN: [
venv\Lib\site-packages\jinja2\lexer.py:565:                    TOKEN_VARIABLE_END,
venv\Lib\site-packages\jinja2\lexer.py:571:            TOKEN_RAW_BEGIN: [
venv\Lib\site-packages\jinja2\lexer.py:578:                    OptionalLStrip(TOKEN_DATA, TOKEN_RAW_END),  # type: ignore
venv\Lib\site-packages\jinja2\lexer.py:584:            TOKEN_LINESTATEMENT_BEGIN: [
venv\Lib\site-packages\jinja2\lexer.py:585:                _Rule(c(r"\s*(\n|$)"), TOKEN_LINESTATEMENT_END, "#pop")
venv\Lib\site-packages\jinja2\lexer.py:589:            TOKEN_LINECOMMENT_BEGIN: [
venv\Lib\site-packages\jinja2\lexer.py:592:                    (TOKEN_LINECOMMENT, TOKEN_LINECOMMENT_END),
venv\Lib\site-packages\jinja2\lexer.py:604:    def tokenize(
venv\Lib\site-packages\jinja2\lexer.py:610:    ) -> TokenStream:
venv\Lib\site-packages\jinja2\lexer.py:611:        """Calls tokeniter + tokenize and wraps it in a token stream."""
venv\Lib\site-packages\jinja2\lexer.py:612:        stream = self.tokeniter(source, name, filename, state)
venv\Lib\site-packages\jinja2\lexer.py:613:        return TokenStream(self.wrap(stream, name, filename), name, filename)
venv\Lib\site-packages\jinja2\lexer.py:620:    ) -> t.Iterator[Token]:
venv\Lib\site-packages\jinja2\lexer.py:621:        """This is called with the stream as returned by `tokenize` and wraps
venv\Lib\site-packages\jinja2\lexer.py:622:        every token in a :class:`Token` and converts the value.
venv\Lib\site-packages\jinja2\lexer.py:624:        for lineno, token, value_str in stream:
venv\Lib\site-packages\jinja2\lexer.py:625:            if token in ignored_tokens:
venv\Lib\site-packages\jinja2\lexer.py:630:            if token == TOKEN_LINESTATEMENT_BEGIN:
venv\Lib\site-packages\jinja2\lexer.py:631:                token = TOKEN_BLOCK_BEGIN
venv\Lib\site-packages\jinja2\lexer.py:632:            elif token == TOKEN_LINESTATEMENT_END:
venv\Lib\site-packages\jinja2\lexer.py:633:                token = TOKEN_BLOCK_END
venv\Lib\site-packages\jinja2\lexer.py:634:            # we are not interested in those tokens in the parser
venv\Lib\site-packages\jinja2\lexer.py:635:            elif token in (TOKEN_RAW_BEGIN, TOKEN_RAW_END):
venv\Lib\site-packages\jinja2\lexer.py:637:            elif token == TOKEN_DATA:
venv\Lib\site-packages\jinja2\lexer.py:639:            elif token == "keyword":
venv\Lib\site-packages\jinja2\lexer.py:640:                token = value_str
venv\Lib\site-packages\jinja2\lexer.py:641:            elif token == TOKEN_NAME:
venv\Lib\site-packages\jinja2\lexer.py:648:            elif token == TOKEN_STRING:
venv\Lib\site-packages\jinja2\lexer.py:659:            elif token == TOKEN_INTEGER:
venv\Lib\site-packages\jinja2\lexer.py:661:            elif token == TOKEN_FLOAT:
venv\Lib\site-packages\jinja2\lexer.py:664:            elif token == TOKEN_OPERATOR:
venv\Lib\site-packages\jinja2\lexer.py:665:                token = operators[value_str]
venv\Lib\site-packages\jinja2\lexer.py:667:            yield Token(lineno, token, value)
venv\Lib\site-packages\jinja2\lexer.py:669:    def tokeniter(
venv\Lib\site-packages\jinja2\lexer.py:676:        """This method tokenizes the text and returns the tokens in a
venv\Lib\site-packages\jinja2\lexer.py:677:        generator. Use this method if you just want to tokenize a template.
venv\Lib\site-packages\jinja2\lexer.py:697:        statetokens = self.rules[stack[-1]]
venv\Lib\site-packages\jinja2\lexer.py:704:            # tokenizer loop
venv\Lib\site-packages\jinja2\lexer.py:705:            for regex, tokens, new_state in statetokens:
venv\Lib\site-packages\jinja2\lexer.py:716:                if balancing_stack and tokens in (
venv\Lib\site-packages\jinja2\lexer.py:717:                    TOKEN_VARIABLE_END,
venv\Lib\site-packages\jinja2\lexer.py:718:                    TOKEN_BLOCK_END,
venv\Lib\site-packages\jinja2\lexer.py:719:                    TOKEN_LINESTATEMENT_END,
venv\Lib\site-packages\jinja2\lexer.py:724:                if isinstance(tokens, tuple):
venv\Lib\site-packages\jinja2\lexer.py:727:                    if isinstance(tokens, OptionalLStrip):
venv\Lib\site-packages\jinja2\lexer.py:747:                            and not m.groupdict().get(TOKEN_VARIABLE_BEGIN)
venv\Lib\site-packages\jinja2\lexer.py:758:                    for idx, token in enumerate(tokens):
venv\Lib\site-packages\jinja2\lexer.py:760:                        if isinstance(token, Failure):
venv\Lib\site-packages\jinja2\lexer.py:761:                            raise token(lineno, filename)
venv\Lib\site-packages\jinja2\lexer.py:763:                        # yield for the current token the first named
venv\Lib\site-packages\jinja2\lexer.py:765:                        elif token == "#bygroup":
venv\Lib\site-packages\jinja2\lexer.py:773:                                    f"{regex!r} wanted to resolve the token dynamically"
venv\Lib\site-packages\jinja2\lexer.py:780:                            if data or token not in ignore_if_empty:
venv\Lib\site-packages\jinja2\lexer.py:781:                                yield lineno, token, data  # type: ignore[misc]
venv\Lib\site-packages\jinja2\lexer.py:786:                # strings as token just are yielded as it.
venv\Lib\site-packages\jinja2\lexer.py:791:                    if tokens == TOKEN_OPERATOR:
venv\Lib\site-packages\jinja2\lexer.py:815:                    if data or tokens not in ignore_if_empty:
venv\Lib\site-packages\jinja2\lexer.py:816:                        yield lineno, tokens, data
venv\Lib\site-packages\jinja2\lexer.py:846:                    statetokens = self.rules[stack[-1]]
venv\Lib\site-packages\jinja2\parser.py:1:"""Parse tokens from the lexer into nodes for the compiler."""
venv\Lib\site-packages\jinja2\parser.py:9:from .lexer import describe_token
venv\Lib\site-packages\jinja2\parser.py:10:from .lexer import describe_token_expr
venv\Lib\site-packages\jinja2\parser.py:62:        self.stream = environment._tokenize(source, name, filename, state)
venv\Lib\site-packages\jinja2\parser.py:74:        self._end_token_stack: t.List[t.Tuple[str, ...]] = []
venv\Lib\site-packages\jinja2\parser.py:93:        end_token_stack: t.List[t.Tuple[str, ...]],
venv\Lib\site-packages\jinja2\parser.py:97:        for exprs in end_token_stack:
venv\Lib\site-packages\jinja2\parser.py:98:            expected.update(map(describe_token_expr, exprs))
venv\Lib\site-packages\jinja2\parser.py:99:        if end_token_stack:
venv\Lib\site-packages\jinja2\parser.py:101:                map(repr, map(describe_token_expr, end_token_stack[-1]))
venv\Lib\site-packages\jinja2\parser.py:137:        self._fail_ut_eof(name, self._end_token_stack, lineno)
venv\Lib\site-packages\jinja2\parser.py:141:        end_tokens: t.Optional[t.Tuple[str, ...]] = None,
venv\Lib\site-packages\jinja2\parser.py:145:        stack = list(self._end_token_stack)
venv\Lib\site-packages\jinja2\parser.py:146:        if end_tokens is not None:
venv\Lib\site-packages\jinja2\parser.py:147:            stack.append(end_tokens)
venv\Lib\site-packages\jinja2\parser.py:169:        token = self.stream.current
venv\Lib\site-packages\jinja2\parser.py:170:        if token.type != "name":
venv\Lib\site-packages\jinja2\parser.py:171:            self.fail("tag name expected", token.lineno)
venv\Lib\site-packages\jinja2\parser.py:172:        self._tag_stack.append(token.value)
venv\Lib\site-packages\jinja2\parser.py:175:            if token.value in _statement_keywords:
venv\Lib\site-packages\jinja2\parser.py:178:            if token.value == "call":
venv\Lib\site-packages\jinja2\parser.py:180:            if token.value == "filter":
venv\Lib\site-packages\jinja2\parser.py:182:            ext = self.extensions.get(token.value)
venv\Lib\site-packages\jinja2\parser.py:186:            # did not work out, remove the token we pushed by accident
venv\Lib\site-packages\jinja2\parser.py:191:            self.fail_unknown_tag(token.value, token.lineno)
venv\Lib\site-packages\jinja2\parser.py:197:        self, end_tokens: t.Tuple[str, ...], drop_needle: bool = False
venv\Lib\site-packages\jinja2\parser.py:199:        """Parse multiple statements into a list until one of the end tokens
venv\Lib\site-packages\jinja2\parser.py:202:        current token is a colon and skips it if there is one.  Then it checks
venv\Lib\site-packages\jinja2\parser.py:203:        for the block end and parses until if one of the `end_tokens` is
venv\Lib\site-packages\jinja2\parser.py:204:        reached.  Per default the active token in the stream at the end of
venv\Lib\site-packages\jinja2\parser.py:205:        the call is the matched end token.  If this is not wanted `drop_needle`
venv\Lib\site-packages\jinja2\parser.py:206:        can be set to `True` and the end token is removed.
venv\Lib\site-packages\jinja2\parser.py:208:        # the first token may be a colon for python compatibility
venv\Lib\site-packages\jinja2\parser.py:212:        # by adding some sort of end of statement token and parsing those here.
venv\Lib\site-packages\jinja2\parser.py:214:        result = self.subparse(end_tokens)
venv\Lib\site-packages\jinja2\parser.py:219:            self.fail_eof(end_tokens)
venv\Lib\site-packages\jinja2\parser.py:263:            token = next(self.stream)
venv\Lib\site-packages\jinja2\parser.py:264:            if token.test("name:elif"):
venv\Lib\site-packages\jinja2\parser.py:268:            elif token.test("name:else"):
venv\Lib\site-packages\jinja2\parser.py:491:            token = self.stream.expect("name")
venv\Lib\site-packages\jinja2\parser.py:492:            target = nodes.Name(token.value, "store", lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:565:            token_type = self.stream.current.type
venv\Lib\site-packages\jinja2\parser.py:566:            if token_type in _compare_operators:
venv\Lib\site-packages\jinja2\parser.py:568:                ops.append(nodes.Operand(token_type, self.parse_math1()))
venv\Lib\site-packages\jinja2\parser.py:626:        token_type = self.stream.current.type
venv\Lib\site-packages\jinja2\parser.py:630:        if token_type == "sub":
venv\Lib\site-packages\jinja2\parser.py:633:        elif token_type == "add":
venv\Lib\site-packages\jinja2\parser.py:646:        token = self.stream.current
venv\Lib\site-packages\jinja2\parser.py:648:        if token.type == "name":
venv\Lib\site-packages\jinja2\parser.py:650:            if token.value in ("true", "false", "True", "False"):
venv\Lib\site-packages\jinja2\parser.py:651:                node = nodes.Const(token.value in ("true", "True"), lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:652:            elif token.value in ("none", "None"):
venv\Lib\site-packages\jinja2\parser.py:653:                node = nodes.Const(None, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:656:                # token is a dot, produce a namespace reference.
venv\Lib\site-packages\jinja2\parser.py:659:                node = nodes.NSRef(token.value, attr.value, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:661:                node = nodes.Name(token.value, "load", lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:662:        elif token.type == "string":
venv\Lib\site-packages\jinja2\parser.py:664:            buf = [token.value]
venv\Lib\site-packages\jinja2\parser.py:665:            lineno = token.lineno
venv\Lib\site-packages\jinja2\parser.py:670:        elif token.type in ("integer", "float"):
venv\Lib\site-packages\jinja2\parser.py:672:            node = nodes.Const(token.value, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:673:        elif token.type == "lparen":
venv\Lib\site-packages\jinja2\parser.py:677:        elif token.type == "lbracket":
venv\Lib\site-packages\jinja2\parser.py:679:        elif token.type == "lbrace":
venv\Lib\site-packages\jinja2\parser.py:682:            self.fail(f"unexpected {describe_token(token)!r}", token.lineno)
venv\Lib\site-packages\jinja2\parser.py:749:                    f" got {describe_token(self.stream.current)!r}"
venv\Lib\site-packages\jinja2\parser.py:755:        token = self.stream.expect("lbracket")
venv\Lib\site-packages\jinja2\parser.py:764:        return nodes.List(items, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:767:        token = self.stream.expect("lbrace")
venv\Lib\site-packages\jinja2\parser.py:779:        return nodes.Dict(items, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:783:            token_type = self.stream.current.type
venv\Lib\site-packages\jinja2\parser.py:784:            if token_type == "dot" or token_type == "lbracket":
venv\Lib\site-packages\jinja2\parser.py:788:            elif token_type == "lparen":
venv\Lib\site-packages\jinja2\parser.py:796:            token_type = self.stream.current.type
venv\Lib\site-packages\jinja2\parser.py:797:            if token_type == "pipe":
venv\Lib\site-packages\jinja2\parser.py:799:            elif token_type == "name" and self.stream.current.value == "is":
venv\Lib\site-packages\jinja2\parser.py:803:            elif token_type == "lparen":
venv\Lib\site-packages\jinja2\parser.py:812:        token = next(self.stream)
venv\Lib\site-packages\jinja2\parser.py:815:        if token.type == "dot":
venv\Lib\site-packages\jinja2\parser.py:816:            attr_token = self.stream.current
venv\Lib\site-packages\jinja2\parser.py:818:            if attr_token.type == "name":
venv\Lib\site-packages\jinja2\parser.py:820:                    node, attr_token.value, "load", lineno=token.lineno
venv\Lib\site-packages\jinja2\parser.py:822:            elif attr_token.type != "integer":
venv\Lib\site-packages\jinja2\parser.py:823:                self.fail("expected name or number", attr_token.lineno)
venv\Lib\site-packages\jinja2\parser.py:824:            arg = nodes.Const(attr_token.value, lineno=attr_token.lineno)
venv\Lib\site-packages\jinja2\parser.py:825:            return nodes.Getitem(node, arg, "load", lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:826:        if token.type == "lbracket":
venv\Lib\site-packages\jinja2\parser.py:836:                arg = nodes.Tuple(args, "load", lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:837:            return nodes.Getitem(node, arg, "load", lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:838:        self.fail("expected subscript expression", token.lineno)
venv\Lib\site-packages\jinja2\parser.py:880:        token = self.stream.expect("lparen")
venv\Lib\site-packages\jinja2\parser.py:889:                self.fail("invalid syntax for function call expression", token.lineno)
venv\Lib\site-packages\jinja2\parser.py:931:        token = self.stream.current
venv\Lib\site-packages\jinja2\parser.py:933:        return nodes.Call(node, args, kwargs, dyn_args, dyn_kwargs, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:941:            token = self.stream.expect("name")
venv\Lib\site-packages\jinja2\parser.py:942:            name = token.value
venv\Lib\site-packages\jinja2\parser.py:953:                node, name, args, kwargs, dyn_args, dyn_kwargs, lineno=token.lineno
venv\Lib\site-packages\jinja2\parser.py:959:        token = next(self.stream)
venv\Lib\site-packages\jinja2\parser.py:990:            node, name, args, kwargs, dyn_args, dyn_kwargs, lineno=token.lineno
venv\Lib\site-packages\jinja2\parser.py:993:            node = nodes.Not(node, lineno=token.lineno)
venv\Lib\site-packages\jinja2\parser.py:997:        self, end_tokens: t.Optional[t.Tuple[str, ...]] = None
venv\Lib\site-packages\jinja2\parser.py:1003:        if end_tokens is not None:
venv\Lib\site-packages\jinja2\parser.py:1004:            self._end_token_stack.append(end_tokens)
venv\Lib\site-packages\jinja2\parser.py:1014:                token = self.stream.current
venv\Lib\site-packages\jinja2\parser.py:1015:                if token.type == "data":
venv\Lib\site-packages\jinja2\parser.py:1016:                    if token.value:
venv\Lib\site-packages\jinja2\parser.py:1017:                        add_data(nodes.TemplateData(token.value, lineno=token.lineno))
venv\Lib\site-packages\jinja2\parser.py:1019:                elif token.type == "variable_begin":
venv\Lib\site-packages\jinja2\parser.py:1023:                elif token.type == "block_begin":
venv\Lib\site-packages\jinja2\parser.py:1026:                    if end_tokens is not None and self.stream.current.test_any(
venv\Lib\site-packages\jinja2\parser.py:1027:                        *end_tokens
venv\Lib\site-packages\jinja2\parser.py:1041:            if end_tokens is not None:
venv\Lib\site-packages\jinja2\parser.py:1042:                self._end_token_stack.pop()
venv\Lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py:381:                    tokens = set(name[len(prefix) :].split("."))
venv\Lib\site-packages\joblib\externals\cloudpickle\cloudpickle.py:382:                    if not tokens - set(code.co_names):
venv\Lib\site-packages\joblib\func_inspect.py:15:from tokenize import open as open_py_source
venv\Lib\site-packages\joblib\memory.py:22:import tokenize
venv\Lib\site-packages\joblib\memory.py:750:                with tokenize.open(source_file) as f:
venv\Lib\site-packages\numpy\f2py\rules.py:340:                           PyObject *capi_keywds,
venv\Lib\site-packages\numpy\f2py\rules.py:351:    if (!PyArg_ParseTupleAndKeywords(capi_args,capi_keywds,\\
venv\Lib\site-packages\numpy\lib\tests\test_loadtxt.py:314:    With the 'bytes' encoding, tokens are encoded prior to being
venv\Lib\site-packages\numpy\lib\tests\test_loadtxt.py:802:    # Check that universal newline support within the tokenizer is not applied
venv\Lib\site-packages\numpy\lib\_format_impl.py:603:    import tokenize
venv\Lib\site-packages\numpy\lib\_format_impl.py:606:    tokens = []
venv\Lib\site-packages\numpy\lib\_format_impl.py:607:    last_token_was_number = False
venv\Lib\site-packages\numpy\lib\_format_impl.py:608:    for token in tokenize.generate_tokens(StringIO(s).readline):
venv\Lib\site-packages\numpy\lib\_format_impl.py:609:        token_type = token[0]
venv\Lib\site-packages\numpy\lib\_format_impl.py:610:        token_string = token[1]
venv\Lib\site-packages\numpy\lib\_format_impl.py:611:        if (last_token_was_number and
venv\Lib\site-packages\numpy\lib\_format_impl.py:612:                token_type == tokenize.NAME and
venv\Lib\site-packages\numpy\lib\_format_impl.py:613:                token_string == "L"):
venv\Lib\site-packages\numpy\lib\_format_impl.py:616:            tokens.append(token)
venv\Lib\site-packages\numpy\lib\_format_impl.py:617:        last_token_was_number = (token_type == tokenize.NUMBER)
venv\Lib\site-packages\numpy\lib\_format_impl.py:618:    return tokenize.untokenize(tokens)
venv\Lib\site-packages\numpy\tests\test_warnings.py:6:import tokenize
venv\Lib\site-packages\numpy\tests\test_warnings.py:74:        # use tokenize to auto-detect encoding on systems where no
venv\Lib\site-packages\numpy\tests\test_warnings.py:76:        with tokenize.open(str(path)) as file:
venv\Lib\site-packages\numpy\_core\arrayprint.py:414:    token = _set_printoptions(*args, **kwargs)
venv\Lib\site-packages\numpy\_core\arrayprint.py:419:        format_options.reset(token)
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:34:        ("set_secret_data_policy", "METH_NOARGS", """
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:35:             PyObject *secret_data =
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:36:                 PyCapsule_New(&secret_data_handler, "mem_handler", NULL);
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:37:             if (secret_data == NULL) {
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:40:             PyObject *old = PyDataMem_SetHandler(secret_data);
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:41:             Py_DECREF(secret_data);
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:46:                 PyCapsule_New(&secret_data_handler, "not_mem_handler", NULL);
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:113:         * of the `secret_data_allocator`. It is provided here for
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:121:        } SecretDataAllocatorFuncs;
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:125:            SecretDataAllocatorFuncs *funcs = (SecretDataAllocatorFuncs *)ctx;
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:135:            SecretDataAllocatorFuncs *funcs = (SecretDataAllocatorFuncs *)ctx;
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:146:            SecretDataAllocatorFuncs *funcs = (SecretDataAllocatorFuncs *)ctx;
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:173:            SecretDataAllocatorFuncs *funcs = (SecretDataAllocatorFuncs *)ctx;
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:193:        static SecretDataAllocatorFuncs secret_data_handler_ctx = {
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:199:        static PyDataMem_Handler secret_data_handler = {
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:200:            "secret_data_allocator",
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:203:                &secret_data_handler_ctx, /* ctx */
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:246:    orig_policy = get_module.set_secret_data_policy()
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:251:    assert get_handler_name(b.base) == 'secret_data_allocator'
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:308:        get_module.set_secret_data_policy()
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:309:        assert get_handler_name() == 'secret_data_allocator'
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:322:        get_module.set_secret_data_policy()
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:323:        assert get_handler_name() == 'secret_data_allocator'
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:353:    get_module.set_secret_data_policy()
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:354:    assert np._core.multiarray.get_handler_name() == 'secret_data_allocator'
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:363:    get_module.set_secret_data_policy()
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:389:    orig_policy = get_module.set_secret_data_policy()
venv\Lib\site-packages\numpy\_core\tests\test_mem_policy.py:392:    assert np._core.multiarray.get_handler_name(b) == 'secret_data_allocator'
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:434:        "_token",
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:440:        self._token = None
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:450:        if self._token is not None:
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:462:        self._token = _extobj_contextvar.set(extobj)
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:465:        _extobj_contextvar.reset(self._token)
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:469:        # because we must store the token per-thread so cannot store it on
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:484:            _token = _extobj_contextvar.set(extobj)
venv\Lib\site-packages\numpy\_core\_ufunc_config.py:489:                _extobj_contextvar.reset(_token)
venv\Lib\site-packages\numpy\_core\_ufunc_config.pyi:44:    __slots__ = "_all", "_call", "_divide", "_invalid", "_over", "_token", "_under"
venv\Lib\site-packages\numpy-2.3.5.dist-info\LICENSE.txt:615:source code form), and must require no special password or key for
venv\Lib\site-packages\numpy-2.3.5.dist-info\METADATA:621:         source code form), and must require no special password or key for
venv\Lib\site-packages\pandas\core\computation\eval.py:6:import tokenize
venv\Lib\site-packages\pandas\core\computation\eval.py:23:from pandas.core.computation.parsing import tokenize_string
venv\Lib\site-packages\pandas\core\computation\eval.py:168:        for toknum, tokval in tokenize_string(expr):
venv\Lib\site-packages\pandas\core\computation\eval.py:169:            if toknum == tokenize.OP and tokval == "@":
venv\Lib\site-packages\pandas\core\computation\expr.py:12:import tokenize
venv\Lib\site-packages\pandas\core\computation\expr.py:44:    tokenize_string,
venv\Lib\site-packages\pandas\core\computation\expr.py:59:        ints correspond to the all caps constants in the tokenize module
venv\Lib\site-packages\pandas\core\computation\expr.py:64:        Either the input or token or the replacement values
venv\Lib\site-packages\pandas\core\computation\expr.py:78:        ints correspond to the all caps constants in the tokenize module
venv\Lib\site-packages\pandas\core\computation\expr.py:83:        Either the input or token or the replacement values
venv\Lib\site-packages\pandas\core\computation\expr.py:86:    if toknum == tokenize.OP:
venv\Lib\site-packages\pandas\core\computation\expr.py:88:            return tokenize.NAME, "and"
venv\Lib\site-packages\pandas\core\computation\expr.py:90:            return tokenize.NAME, "or"
venv\Lib\site-packages\pandas\core\computation\expr.py:102:        ints correspond to the all caps constants in the tokenize module
venv\Lib\site-packages\pandas\core\computation\expr.py:107:        Either the input or token or the replacement values
venv\Lib\site-packages\pandas\core\computation\expr.py:112:    ``'__pd_eval_local_a'`` by telling the tokenizer that ``__pd_eval_local_``
venv\Lib\site-packages\pandas\core\computation\expr.py:113:    is a ``tokenize.OP`` and to replace the ``'@'`` symbol with it.
venv\Lib\site-packages\pandas\core\computation\expr.py:116:    if toknum == tokenize.OP and tokval == "@":
venv\Lib\site-packages\pandas\core\computation\expr.py:117:        return tokenize.OP, LOCAL_TAG
venv\Lib\site-packages\pandas\core\computation\expr.py:143:    Compose a collection of tokenization functions.
venv\Lib\site-packages\pandas\core\computation\expr.py:164:    the ``tokenize`` module and ``tokval`` is a string.
venv\Lib\site-packages\pandas\core\computation\expr.py:167:    return tokenize.untokenize(f(x) for x in tokenize_string(source))
venv\Lib\site-packages\pandas\core\computation\ops.py:507:        The token used to represent the operator.
venv\Lib\site-packages\pandas\core\computation\ops.py:514:        * If no function associated with the passed operator token is found.
venv\Lib\site-packages\pandas\core\computation\parsing.py:8:import token
venv\Lib\site-packages\pandas\core\computation\parsing.py:9:import tokenize
venv\Lib\site-packages\pandas\core\computation\parsing.py:18:# A token value Python's tokenizer probably will never use.
venv\Lib\site-packages\pandas\core\computation\parsing.py:34:        This can happen if there is a hashtag in the name, as the tokenizer will
venv\Lib\site-packages\pandas\core\computation\parsing.py:42:    # EXACT_TOKEN_TYPES contains these special characters
venv\Lib\site-packages\pandas\core\computation\parsing.py:43:    # token.tok_name contains a readable description of the replacement string.
venv\Lib\site-packages\pandas\core\computation\parsing.py:45:        char: f"_{token.tok_name[tokval]}_"
venv\Lib\site-packages\pandas\core\computation\parsing.py:46:        for char, tokval in (tokenize.EXACT_TOKEN_TYPES.items())
venv\Lib\site-packages\pandas\core\computation\parsing.py:78:    is a backtick quoted token it will processed by
venv\Lib\site-packages\pandas\core\computation\parsing.py:86:        ints correspond to the all caps constants in the tokenize module
venv\Lib\site-packages\pandas\core\computation\parsing.py:91:        Either the input or token or the replacement values
venv\Lib\site-packages\pandas\core\computation\parsing.py:95:        return tokenize.NAME, create_valid_python_identifier(tokval)
venv\Lib\site-packages\pandas\core\computation\parsing.py:116:        Returns the name after tokenizing and cleaning.
venv\Lib\site-packages\pandas\core\computation\parsing.py:121:        In that case :func:`tokenize_string` raises a SyntaxError.
venv\Lib\site-packages\pandas\core\computation\parsing.py:125:        an error will be raised by :func:`tokenize_backtick_quoted_string` instead,
venv\Lib\site-packages\pandas\core\computation\parsing.py:129:        tokenized = tokenize_string(f"`{name}`")
venv\Lib\site-packages\pandas\core\computation\parsing.py:130:        tokval = next(tokenized)[1]
venv\Lib\site-packages\pandas\core\computation\parsing.py:136:def tokenize_backtick_quoted_string(
venv\Lib\site-packages\pandas\core\computation\parsing.py:137:    token_generator: Iterator[tokenize.TokenInfo], source: str, string_start: int
venv\Lib\site-packages\pandas\core\computation\parsing.py:140:    Creates a token from a backtick quoted string.
venv\Lib\site-packages\pandas\core\computation\parsing.py:142:    Moves the token_generator forwards till right after the next backtick.
venv\Lib\site-packages\pandas\core\computation\parsing.py:146:    token_generator : Iterator[tokenize.TokenInfo]
venv\Lib\site-packages\pandas\core\computation\parsing.py:147:        The generator that yields the tokens of the source string (Tuple[int, str]).
venv\Lib\site-packages\pandas\core\computation\parsing.py:148:        The generator is at the first token after the backtick (`)
venv\Lib\site-packages\pandas\core\computation\parsing.py:159:        The token that represents the backtick quoted string.
venv\Lib\site-packages\pandas\core\computation\parsing.py:162:    for _, tokval, start, _, _ in token_generator:
venv\Lib\site-packages\pandas\core\computation\parsing.py:170:def tokenize_string(source: str) -> Iterator[tuple[int, str]]:
venv\Lib\site-packages\pandas\core\computation\parsing.py:172:    Tokenize a Python source code string.
venv\Lib\site-packages\pandas\core\computation\parsing.py:182:        An iterator yielding all tokens with only toknum and tokval (Tuple[ing, str]).
venv\Lib\site-packages\pandas\core\computation\parsing.py:185:    token_generator = tokenize.generate_tokens(line_reader)
venv\Lib\site-packages\pandas\core\computation\parsing.py:187:    # Loop over all tokens till a backtick (`) is found.
venv\Lib\site-packages\pandas\core\computation\parsing.py:188:    # Then, take all tokens till the next backtick to form a backtick quoted string
venv\Lib\site-packages\pandas\core\computation\parsing.py:189:    for toknum, tokval, start, _, _ in token_generator:
venv\Lib\site-packages\pandas\core\computation\parsing.py:192:                yield tokenize_backtick_quoted_string(
venv\Lib\site-packages\pandas\core\computation\parsing.py:193:                    token_generator, source, string_start=start[1] + 1
venv\Lib\site-packages\pandas\core\shared_docs.py:445:    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs
venv\Lib\site-packages\pandas\io\formats\css.py:45:            value (str): String token for property
venv\Lib\site-packages\pandas\io\formats\css.py:51:        tokens = value.split()
venv\Lib\site-packages\pandas\io\formats\css.py:53:            mapping = self.SIDE_SHORTHANDS[len(tokens)]
venv\Lib\site-packages\pandas\io\formats\css.py:62:            yield prop_fmt.format(key), tokens[idx]
venv\Lib\site-packages\pandas\io\formats\css.py:98:        tokens = value.split()
venv\Lib\site-packages\pandas\io\formats\css.py:99:        if len(tokens) == 0 or len(tokens) > 3:
venv\Lib\site-packages\pandas\io\formats\css.py:101:                f'Too many tokens provided to "{prop}" (expected 1-3)',
venv\Lib\site-packages\pandas\io\formats\css.py:112:        for token in tokens:
venv\Lib\site-packages\pandas\io\formats\css.py:113:            if token.lower() in self.BORDER_STYLES:
venv\Lib\site-packages\pandas\io\formats\css.py:114:                border_declarations[f"border{side}-style"] = token
venv\Lib\site-packages\pandas\io\formats\css.py:115:            elif any(ratio in token.lower() for ratio in self.BORDER_WIDTH_RATIOS):
venv\Lib\site-packages\pandas\io\formats\css.py:116:                border_declarations[f"border{side}-width"] = token
venv\Lib\site-packages\pandas\io\formats\css.py:118:                border_declarations[f"border{side}-color"] = token
venv\Lib\site-packages\pandas\io\formats\css.py:401:        In a future version may generate parsed tokens from tinycss/tinycss2
venv\Lib\site-packages\pandas\tests\computation\test_eval.py:1930:def test_query_token(engine, column):
venv\Lib\site-packages\pandas\tests\io\conftest.py:67:    monkeypatch.setenv("AWS_SECRET_ACCESS_KEY", "foobar_secret")
venv\Lib\site-packages\pandas\tests\io\parser\common\test_verbose.py:43:        assert "Tokenization took:" in captured.out
venv\Lib\site-packages\pandas\tests\io\parser\common\test_verbose.py:78:        assert "Tokenization took:" in captured.out
venv\Lib\site-packages\pandas\tests\io\parser\test_c_parser_only.py:41:    # buffer overflows in tokenizer.c
venv\Lib\site-packages\pandas\tests\io\parser\test_c_parser_only.py:280:def test_tokenize_CR_with_quoting(c_parser_only):
venv\Lib\site-packages\pandas\tests\io\parser\test_c_parser_only.py:323:    # `tokenizer.c`. Sometimes the test fails on `segfault`, other
venv\Lib\site-packages\pandas\tests\io\parser\test_textreader.py:134:        msg = r"Error tokenizing data\. C error: Expected 3 fields in line 4, saw 4"
venv\Lib\site-packages\pandas\tests\io\parser\test_unsupported.py:74:        msg = "Error tokenizing data"
venv\Lib\site-packages\pandas\tests\io\test_sql.py:4233:        tokens = line.split(" ")
venv\Lib\site-packages\pandas\tests\io\test_sql.py:4234:        if len(tokens) == 2 and tokens[0] == "A":
venv\Lib\site-packages\pandas\tests\io\test_sql.py:4235:            assert tokens[1] == "DATETIME"
venv\Lib\site-packages\pandas\tests\strings\test_case_justify.py:377:    # equal to width, one word greater than width, multiple tokens with
venv\Lib\site-packages\pandas\_config\config.py:523:    import tokenize
venv\Lib\site-packages\pandas\_config\config.py:540:        if not re.match("^" + tokenize.Name + "$", k):
venv\Lib\site-packages\pandas\_libs\parsers.pyi:36:        tokenize_chunksize: int = ...,  # int64_t
venv\Lib\site-packages\pip\_internal\cli\cmdoptions.py:274:    help="Specify a proxy in the form scheme://[user:passwd@]proxy.server:port.",
venv\Lib\site-packages\pip\_internal\models\direct_url.py:180:        """url with user:password part removed unless it is formed with
venv\Lib\site-packages\pip\_internal\models\link.py:413:            # includes a username and password.
venv\Lib\site-packages\pip\_internal\network\auth.py:28:    ask_password,
venv\Lib\site-packages\pip\_internal\network\auth.py:42:    password: str
venv\Lib\site-packages\pip\_internal\network\auth.py:56:    def save_auth_info(self, url: str, username: str, password: str) -> None: ...
venv\Lib\site-packages\pip\_internal\network\auth.py:67:    def save_auth_info(self, url: str, username: str, password: str) -> None:
venv\Lib\site-packages\pip\_internal\network\auth.py:89:                return cred.username, cred.password
venv\Lib\site-packages\pip\_internal\network\auth.py:93:            logger.debug("Getting password from keyring for %s", url)
venv\Lib\site-packages\pip\_internal\network\auth.py:94:            password = self.keyring.get_password(url, username)
venv\Lib\site-packages\pip\_internal\network\auth.py:95:            if password:
venv\Lib\site-packages\pip\_internal\network\auth.py:96:                return username, password
venv\Lib\site-packages\pip\_internal\network\auth.py:99:    def save_auth_info(self, url: str, username: str, password: str) -> None:
venv\Lib\site-packages\pip\_internal\network\auth.py:100:        self.keyring.set_password(url, username, password)
venv\Lib\site-packages\pip\_internal\network\auth.py:121:            password = self._get_password(url, username)
venv\Lib\site-packages\pip\_internal\network\auth.py:122:            if password is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:123:                return username, password
venv\Lib\site-packages\pip\_internal\network\auth.py:126:    def save_auth_info(self, url: str, username: str, password: str) -> None:
venv\Lib\site-packages\pip\_internal\network\auth.py:127:        return self._set_password(url, username, password)
venv\Lib\site-packages\pip\_internal\network\auth.py:129:    def _get_password(self, service_name: str, username: str) -> Optional[str]:
venv\Lib\site-packages\pip\_internal\network\auth.py:130:        """Mirror the implementation of keyring.get_password using cli"""
venv\Lib\site-packages\pip\_internal\network\auth.py:147:    def _set_password(self, service_name: str, username: str, password: str) -> None:
venv\Lib\site-packages\pip\_internal\network\auth.py:148:        """Mirror the implementation of keyring.set_password using cli"""
venv\Lib\site-packages\pip\_internal\network\auth.py:155:            input=f"{password}{os.linesep}".encode(),
venv\Lib\site-packages\pip\_internal\network\auth.py:234:        self.passwords: Dict[str, AuthInfo] = {}
venv\Lib\site-packages\pip\_internal\network\auth.py:293:        The provided url should have had its username and password
venv\Lib\site-packages\pip\_internal\network\auth.py:344:        url, netloc, url_user_password = split_auth_netloc_from_url(
venv\Lib\site-packages\pip\_internal\network\auth.py:349:        username, password = url_user_password
venv\Lib\site-packages\pip\_internal\network\auth.py:350:        if username is not None and password is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:352:            return url_user_password
venv\Lib\site-packages\pip\_internal\network\auth.py:360:                index_url, _, index_url_user_password = index_info
venv\Lib\site-packages\pip\_internal\network\auth.py:364:        if index_url and index_url_user_password[0] is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:365:            username, password = index_url_user_password
venv\Lib\site-packages\pip\_internal\network\auth.py:366:            if username is not None and password is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:368:                return index_url_user_password
venv\Lib\site-packages\pip\_internal\network\auth.py:377:        # If we don't have a password and keyring is available, use it.
venv\Lib\site-packages\pip\_internal\network\auth.py:390:        return username, password
venv\Lib\site-packages\pip\_internal\network\auth.py:400:        Returns (url_without_credentials, username, password). Note
venv\Lib\site-packages\pip\_internal\network\auth.py:402:        function may return a different username and password.
venv\Lib\site-packages\pip\_internal\network\auth.py:407:        username, password = self._get_new_credentials(original_url)
venv\Lib\site-packages\pip\_internal\network\auth.py:410:        # Do this if either the username or the password is missing.
venv\Lib\site-packages\pip\_internal\network\auth.py:412:        # the username in the index url, but the password comes from keyring.
venv\Lib\site-packages\pip\_internal\network\auth.py:413:        if (username is None or password is None) and netloc in self.passwords:
venv\Lib\site-packages\pip\_internal\network\auth.py:414:            un, pw = self.passwords[netloc]
venv\Lib\site-packages\pip\_internal\network\auth.py:418:                username, password = un, pw
venv\Lib\site-packages\pip\_internal\network\auth.py:420:        if username is not None or password is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:421:            # Convert the username and password if they're None, so that
venv\Lib\site-packages\pip\_internal\network\auth.py:426:            password = password or ""
venv\Lib\site-packages\pip\_internal\network\auth.py:429:            self.passwords[netloc] = (username, password)
venv\Lib\site-packages\pip\_internal\network\auth.py:433:            (username is not None and password is not None)
venv\Lib\site-packages\pip\_internal\network\auth.py:435:            or (username is None and password is None)
venv\Lib\site-packages\pip\_internal\network\auth.py:438:        return url, username, password
venv\Lib\site-packages\pip\_internal\network\auth.py:442:        url, username, password = self._get_url_and_credentials(req.url)
venv\Lib\site-packages\pip\_internal\network\auth.py:447:        if username is not None and password is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:449:            req = HTTPBasicAuth(username, password)(req)
venv\Lib\site-packages\pip\_internal\network\auth.py:457:    def _prompt_for_password(
venv\Lib\site-packages\pip\_internal\network\auth.py:467:        password = ask_password("Password: ")
venv\Lib\site-packages\pip\_internal\network\auth.py:468:        return username, password, True
venv\Lib\site-packages\pip\_internal\network\auth.py:471:    def _should_save_password_to_keyring(self) -> bool:
venv\Lib\site-packages\pip\_internal\network\auth.py:486:        username, password = None, None
venv\Lib\site-packages\pip\_internal\network\auth.py:490:            username, password = self._get_new_credentials(
venv\Lib\site-packages\pip\_internal\network\auth.py:497:        if not self.prompting and not username and not password:
venv\Lib\site-packages\pip\_internal\network\auth.py:502:        # Prompt the user for a new username and password
venv\Lib\site-packages\pip\_internal\network\auth.py:504:        if not username and not password:
venv\Lib\site-packages\pip\_internal\network\auth.py:505:            username, password, save = self._prompt_for_password(parsed.netloc)
venv\Lib\site-packages\pip\_internal\network\auth.py:507:        # Store the new username and password to use for future requests
venv\Lib\site-packages\pip\_internal\network\auth.py:509:        if username is not None and password is not None:
venv\Lib\site-packages\pip\_internal\network\auth.py:510:            self.passwords[parsed.netloc] = (username, password)
venv\Lib\site-packages\pip\_internal\network\auth.py:512:            # Prompt to save the password to keyring
venv\Lib\site-packages\pip\_internal\network\auth.py:513:            if save and self._should_save_password_to_keyring():
venv\Lib\site-packages\pip\_internal\network\auth.py:517:                    password=password,
venv\Lib\site-packages\pip\_internal\network\auth.py:527:        # Add our new username and password to the request
venv\Lib\site-packages\pip\_internal\network\auth.py:528:        req = HTTPBasicAuth(username or "", password or "")(resp.request)
venv\Lib\site-packages\pip\_internal\network\auth.py:563:                    creds.url, creds.username, creds.password
venv\Lib\site-packages\pip\_internal\req\req_file.py:458:    tokens = line.split(" ")
venv\Lib\site-packages\pip\_internal\req\req_file.py:460:    options = tokens[:]
venv\Lib\site-packages\pip\_internal\req\req_file.py:461:    for token in tokens:
venv\Lib\site-packages\pip\_internal\req\req_file.py:462:        if token.startswith("-") or token.startswith("--"):
venv\Lib\site-packages\pip\_internal\req\req_file.py:465:            args.append(token)
venv\Lib\site-packages\pip\_internal\utils\misc.py:246:def ask_password(message: str) -> str:
venv\Lib\site-packages\pip\_internal\utils\misc.py:247:    """Ask for a password interactively."""
venv\Lib\site-packages\pip\_internal\utils\misc.py:437:    Returns: (netloc, (username, password)).
venv\Lib\site-packages\pip\_internal\utils\misc.py:444:    # the password attribute of urlsplit()'s return value).
venv\Lib\site-packages\pip\_internal\utils\misc.py:450:        # using the password attribute of the return value)
venv\Lib\site-packages\pip\_internal\utils\misc.py:468:        - "accesstoken@example.com" returns "****@example.com"
venv\Lib\site-packages\pip\_internal\utils\misc.py:470:    netloc, (user, password) = split_auth_from_netloc(netloc)
venv\Lib\site-packages\pip\_internal\utils\misc.py:473:    if password is None:
venv\Lib\site-packages\pip\_internal\utils\misc.py:475:        password = ""
venv\Lib\site-packages\pip\_internal\utils\misc.py:478:        password = ":****"
venv\Lib\site-packages\pip\_internal\utils\misc.py:479:    return f"{user}{password}@{netloc}"
venv\Lib\site-packages\pip\_internal\utils\misc.py:516:    Returns: (url_without_auth, netloc, (username, password))
venv\Lib\site-packages\pip\_internal\utils\misc.py:523:    """Return a copy of url with 'username:password@' removed."""
venv\Lib\site-packages\pip\_internal\utils\misc.py:530:    """Replace the password in a given url with ****."""
venv\Lib\site-packages\pip\_internal\utils\misc.py:535:    """Replace the password in a given requirement url with ****."""
venv\Lib\site-packages\pip\_internal\utils\misc.py:543:    secret: str
venv\Lib\site-packages\pip\_internal\utils\misc.py:559:        return self.secret == other.secret
venv\Lib\site-packages\pip\_internal\utils\setuptools_build.py:20:    import os, sys, tokenize
venv\Lib\site-packages\pip\_internal\utils\setuptools_build.py:37:        with tokenize.open(__file__) as f:
venv\Lib\site-packages\pip\_internal\utils\subprocess.py:53:    return [arg.secret if isinstance(arg, HiddenText) else arg for arg in args]
venv\Lib\site-packages\pip\_internal\utils\wheel.py:73:        # and RuntimeError for password-protected files
venv\Lib\site-packages\pip\_internal\vcs\mercurial.py:65:            config.set("paths", "default", url.secret)
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:79:        --username and --password options instead of via the URL.
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:82:            # The --username and --password options can't be used for
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:98:        username: Optional[str], password: Optional[HiddenText]
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:103:        if password:
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:104:            extra_args += ["--password", password]
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:161:                # is being used to prompt for passwords, because passwords
venv\Lib\site-packages\pip\_internal\vcs\subversion.py:270:        # the user can be prompted for a password, if required.
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:370:        information can be provided via the --username and --password options
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:374:        Returns: (netloc, (username, password)).
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:384:        Returns: (url, rev, (username, password)).
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:410:        username: Optional[str], password: Optional[HiddenText]
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:422:        secret_url, rev, user_pass = self.get_url_rev_and_auth(url.secret)
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:423:        username, secret_password = user_pass
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:424:        password: Optional[HiddenText] = None
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:425:        if secret_password is not None:
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:426:            password = hide_value(secret_password)
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:427:        extra_args = self.make_rev_args(username, password)
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:430:        return hide_url(secret_url), rev_options
venv\Lib\site-packages\pip\_internal\vcs\versioncontrol.py:508:            if self.compare_urls(existing_url, url.secret):
venv\Lib\site-packages\pip\_vendor\distlib\compat.py:37:                         HTTPBasicAuthHandler, HTTPPasswordMgr, HTTPHandler,
venv\Lib\site-packages\pip\_vendor\distlib\compat.py:53:    # """splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'."""
venv\Lib\site-packages\pip\_vendor\distlib\compat.py:74:                                HTTPPasswordMgr, HTTPHandler,
venv\Lib\site-packages\pip\_vendor\distlib\compat.py:365:    from tokenize import detect_encoding
venv\Lib\site-packages\pip\_vendor\distlib\compat.py:372:        """Imitates get_normal_name in tokenizer.c."""
venv\Lib\site-packages\pip\_vendor\distlib\compat.py:386:        in the same way as the tokenize() generator.
venv\Lib\site-packages\pip\_vendor\distlib\index.py:19:from .compat import (HTTPBasicAuthHandler, Request, HTTPPasswordMgr,
venv\Lib\site-packages\pip\_vendor\distlib\index.py:49:        self.password_handler = None
venv\Lib\site-packages\pip\_vendor\distlib\index.py:55:            # prompting for passwords
venv\Lib\site-packages\pip\_vendor\distlib\index.py:77:        ``username``, ``password``, ``realm`` and ``url`` attributes from the
venv\Lib\site-packages\pip\_vendor\distlib\index.py:83:        self.password = cfg.get('password')
venv\Lib\site-packages\pip\_vendor\distlib\index.py:90:        ``password`` attributes before calling this method.
venv\Lib\site-packages\pip\_vendor\distlib\index.py:98:        Check that ``username`` and ``password`` have been set, and raise an
venv\Lib\site-packages\pip\_vendor\distlib\index.py:101:        if self.username is None or self.password is None:
venv\Lib\site-packages\pip\_vendor\distlib\index.py:102:            raise DistlibException('username and password must be set')
venv\Lib\site-packages\pip\_vendor\distlib\index.py:103:        pm = HTTPPasswordMgr()
venv\Lib\site-packages\pip\_vendor\distlib\index.py:105:        pm.add_password(self.realm, netloc, self.username, self.password)
venv\Lib\site-packages\pip\_vendor\distlib\index.py:106:        self.password_handler = HTTPBasicAuthHandler(pm)
venv\Lib\site-packages\pip\_vendor\distlib\index.py:146:    def get_sign_command(self, filename, signer, sign_password, keystore=None):  # pragma: no cover
venv\Lib\site-packages\pip\_vendor\distlib\index.py:152:        :param sign_password: The passphrase for the signer's
venv\Lib\site-packages\pip\_vendor\distlib\index.py:165:        if sign_password is not None:
venv\Lib\site-packages\pip\_vendor\distlib\index.py:209:    def sign_file(self, filename, signer, sign_password, keystore=None):  # pragma: no cover
venv\Lib\site-packages\pip\_vendor\distlib\index.py:215:        :param sign_password: The passphrase for the signer's
venv\Lib\site-packages\pip\_vendor\distlib\index.py:223:        cmd, sig_file = self.get_sign_command(filename, signer, sign_password,
venv\Lib\site-packages\pip\_vendor\distlib\index.py:226:                                              sign_password.encode('utf-8'))
venv\Lib\site-packages\pip\_vendor\distlib\index.py:232:    def upload_file(self, metadata, filename, signer=None, sign_password=None,
venv\Lib\site-packages\pip\_vendor\distlib\index.py:241:        :param sign_password: The passphrase for the signer's
venv\Lib\site-packages\pip\_vendor\distlib\index.py:265:                sig_file = self.sign_file(filename, signer, sign_password,
venv\Lib\site-packages\pip\_vendor\distlib\index.py:451:        if self.password_handler:
venv\Lib\site-packages\pip\_vendor\distlib\index.py:452:            handlers.append(self.password_handler)
venv\Lib\site-packages\pip\_vendor\distlib\util.py:816:    username = password = None
venv\Lib\site-packages\pip\_vendor\distlib\util.py:822:            username, password = prefix.split(':', 1)
venv\Lib\site-packages\pip\_vendor\distlib\util.py:825:    if password:
venv\Lib\site-packages\pip\_vendor\distlib\util.py:826:        password = unquote(password)
venv\Lib\site-packages\pip\_vendor\distlib\util.py:827:    return username, password, netloc
venv\Lib\site-packages\pip\_vendor\distlib\util.py:1829:                                             ('password', None)):
venv\Lib\site-packages\pip\_vendor\distlib\util.py:1851:                    'password': config.get(server, 'password'),
venv\Lib\site-packages\pip\_vendor\distlib\util.py:1858:    def update(self, username, password):
venv\Lib\site-packages\pip\_vendor\distlib\util.py:1866:        config.set('pypi', 'password', password)
venv\Lib\site-packages\pip\_vendor\distlib\util.py:1879:    PyPIRCFile().update(index.username, index.password)
venv\Lib\site-packages\pip\_vendor\distro\distro.py:1121:        tokens = list(lexer)
venv\Lib\site-packages\pip\_vendor\distro\distro.py:1122:        for token in tokens:
venv\Lib\site-packages\pip\_vendor\distro\distro.py:1126:            # stripped, etc.), so the tokens are now either:
venv\Lib\site-packages\pip\_vendor\distro\distro.py:1129:            # Ignore any tokens that are not variable assignments
venv\Lib\site-packages\pip\_vendor\distro\distro.py:1130:            if "=" in token:
venv\Lib\site-packages\pip\_vendor\distro\distro.py:1131:                k, v = token.split("=", 1)
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:67:    # Pad any parentheses so tokenization can be achieved by merely splitting on
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:81:    tokens = license_expression.split()
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:86:    python_tokens = []
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:87:    for token in tokens:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:88:        if token not in {"or", "and", "with", "(", ")"}:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:89:            python_tokens.append("False")
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:90:        elif token == "with":
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:91:            python_tokens.append("or")
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:92:        elif token == "(" and python_tokens and python_tokens[-1] not in {"or", "and"}:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:96:            python_tokens.append(token)
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:98:    python_expression = " ".join(python_tokens)
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:109:    normalized_tokens = []
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:110:    for token in tokens:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:111:        if token in {"or", "and", "with", "(", ")"}:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:112:            normalized_tokens.append(token.upper())
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:115:        if normalized_tokens and normalized_tokens[-1] == "WITH":
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:116:            if token not in EXCEPTIONS:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:117:                message = f"Unknown license exception: {token!r}"
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:120:            normalized_tokens.append(EXCEPTIONS[token]["id"])
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:122:            if token.endswith("+"):
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:123:                final_token = token[:-1]
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:126:                final_token = token
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:129:            if final_token.startswith("licenseref-"):
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:130:                if not license_ref_allowed.match(final_token):
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:131:                    message = f"Invalid licenseref: {final_token!r}"
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:133:                normalized_tokens.append(license_refs[final_token] + suffix)
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:135:                if final_token not in LICENSES:
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:136:                    message = f"Unknown license: {final_token!r}"
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:138:                normalized_tokens.append(LICENSES[final_token]["id"] + suffix)
venv\Lib\site-packages\pip\_vendor\packaging\licenses\__init__.py:140:    normalized_expression = " ".join(normalized_tokens)
venv\Lib\site-packages\pip\_vendor\packaging\markers.py:15:from ._tokenizer import ParserSyntaxError
venv\Lib\site-packages\pip\_vendor\packaging\requirements.py:9:from ._tokenizer import ParserSyntaxError
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:12:from ._tokenizer import DEFAULT_RULES, Tokenizer
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:62:    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:65:def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:69:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:71:    name_token = tokenizer.expect(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:74:    name = name_token.text
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:75:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:77:    extras = _parse_extras(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:78:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:80:    url, specifier, marker = _parse_requirement_details(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:81:    tokenizer.expect("END", expected="end of dependency specifier")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:87:    tokenizer: Tokenizer,
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:98:    if tokenizer.check("AT"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:99:        tokenizer.read()
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:100:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:102:        url_start = tokenizer.position
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:103:        url = tokenizer.expect("URL", expected="URL after @").text
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:104:        if tokenizer.check("END", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:107:        tokenizer.expect("WS", expected="whitespace after URL")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:110:        if tokenizer.check("END", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:114:            tokenizer, span_start=url_start, after="URL and whitespace"
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:117:        specifier_start = tokenizer.position
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:118:        specifier = _parse_specifier(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:119:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:121:        if tokenizer.check("END", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:125:            tokenizer,
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:138:    tokenizer: Tokenizer, *, span_start: int, after: str
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:144:    if not tokenizer.check("SEMICOLON"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:145:        tokenizer.raise_syntax_error(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:149:    tokenizer.read()
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:151:    marker = _parse_marker(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:152:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:157:def _parse_extras(tokenizer: Tokenizer) -> list[str]:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:161:    if not tokenizer.check("LEFT_BRACKET", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:164:    with tokenizer.enclosing_tokens(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:169:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:170:        extras = _parse_extras_list(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:171:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:176:def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:182:    if not tokenizer.check("IDENTIFIER"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:185:    extras.append(tokenizer.read().text)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:188:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:189:        if tokenizer.check("IDENTIFIER", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:190:            tokenizer.raise_syntax_error("Expected comma between extra names")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:191:        elif not tokenizer.check("COMMA"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:194:        tokenizer.read()
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:195:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:197:        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:198:        extras.append(extra_token.text)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:203:def _parse_specifier(tokenizer: Tokenizer) -> str:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:208:    with tokenizer.enclosing_tokens(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:213:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:214:        parsed_specifiers = _parse_version_many(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:215:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:220:def _parse_version_many(tokenizer: Tokenizer) -> str:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:225:    while tokenizer.check("SPECIFIER"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:226:        span_start = tokenizer.position
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:227:        parsed_specifiers += tokenizer.read().text
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:228:        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:229:            tokenizer.raise_syntax_error(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:232:                span_end=tokenizer.position + 1,
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:234:        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:235:            tokenizer.raise_syntax_error(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:238:                span_end=tokenizer.position,
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:240:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:241:        if not tokenizer.check("COMMA"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:243:        parsed_specifiers += tokenizer.read().text
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:244:        tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:253:    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:256:def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:257:    retval = _parse_marker(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:258:    tokenizer.expect("END", expected="end of marker expression")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:262:def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:266:    expression = [_parse_marker_atom(tokenizer)]
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:267:    while tokenizer.check("BOOLOP"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:268:        token = tokenizer.read()
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:269:        expr_right = _parse_marker_atom(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:270:        expression.extend((token.text, expr_right))
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:274:def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:280:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:281:    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:282:        with tokenizer.enclosing_tokens(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:287:            tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:288:            marker: MarkerAtom = _parse_marker(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:289:            tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:291:        marker = _parse_marker_item(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:292:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:296:def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:300:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:301:    marker_var_left = _parse_marker_var(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:302:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:303:    marker_op = _parse_marker_op(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:304:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:305:    marker_var_right = _parse_marker_var(tokenizer)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:306:    tokenizer.consume("WS")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:310:def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:314:    if tokenizer.check("VARIABLE"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:315:        return process_env_var(tokenizer.read().text.replace(".", "_"))
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:316:    elif tokenizer.check("QUOTED_STRING"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:317:        return process_python_str(tokenizer.read().text)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:319:        tokenizer.raise_syntax_error(
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:336:def _parse_marker_op(tokenizer: Tokenizer) -> Op:
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:340:    if tokenizer.check("IN"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:341:        tokenizer.read()
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:343:    elif tokenizer.check("NOT"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:344:        tokenizer.read()
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:345:        tokenizer.expect("WS", expected="whitespace after 'not'")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:346:        tokenizer.expect("IN", expected="'in' after 'not'")
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:348:    elif tokenizer.check("OP"):
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:349:        return Op(tokenizer.read().text)
venv\Lib\site-packages\pip\_vendor\packaging\_parser.py:351:        return tokenizer.raise_syntax_error(
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:12:class Token:
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:90:class Tokenizer:
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:91:    """Context-sensitive token parsing.
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:93:    Provides methods to examine the input stream to check whether the next token
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:107:        self.next_token: Token | None = None
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:111:        """Move beyond provided token name, if at current position."""
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:116:        """Check whether the next token has the provided name.
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:118:        By default, if the check succeeds, the token *must* be read before
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:119:        another check. If `peek` is set to `True`, the token is not loaded and
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:123:            self.next_token is None
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:124:        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:125:        assert name in self.rules, f"Unknown token name: {name!r}"
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:133:            self.next_token = Token(name, match[0], self.position)
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:136:    def expect(self, name: str, *, expected: str) -> Token:
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:137:        """Expect a certain token name next, failing with a syntax error otherwise.
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:139:        The token is *not* read.
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:145:    def read(self) -> Token:
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:146:        """Consume the next token and return it."""
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:147:        token = self.next_token
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:148:        assert token is not None
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:150:        self.position += len(token.text)
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:151:        self.next_token = None
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:153:        return token
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:174:    def enclosing_tokens(
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:175:        self, open_token: str, close_token: str, *, around: str
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:177:        if self.check(open_token):
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:188:        if not self.check(close_token):
venv\Lib\site-packages\pip\_vendor\packaging\_tokenizer.py:190:                f"Expected matching {close_token} for {open_token}, after {around}",
venv\Lib\site-packages\pip\_vendor\pygments\cmdline.py:548:        help='Add a filter to the token stream.  (Query names with -L.) '
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:14:from pip._vendor.pygments.token import String, Comment, Keyword, Name, Error, Whitespace, \
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:15:    string_to_tokentype
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:716:    """Highlight a normal Name (and Name.*) token with a different token type.
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:722:            tokentype=Name.Function,
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:726:    as functions. `Name.Function` is the default token type.
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:731:      A list of names that should be given the different token type.
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:733:    `tokentype` : TokenType or string
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:734:      A token type or a string containing a token type name that is
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:742:        tokentype = options.get('tokentype')
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:743:        if tokentype:
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:744:            self.tokentype = string_to_tokentype(tokentype)
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:746:            self.tokentype = Name.Function
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:751:                yield self.tokentype, value
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:756:class ErrorToken(Exception):
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:760:class RaiseOnErrorTokenFilter(Filter):
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:761:    """Raise an exception when the lexer generates an error token.
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:767:      The default is `pygments.filters.ErrorToken`.
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:774:        self.exception = options.get('excclass', ErrorToken)
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:811:    `wstokentype` : bool
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:812:      If true, give whitespace the special `Whitespace` token type.  This allows
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:834:        self.wstt = get_bool_opt(options, 'wstokentype', True)
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:896:            # Remove ``left`` tokens from first line, ``n`` from all others.
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:907:class TokenMergeFilter(Filter):
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:908:    """Merges consecutive tokens with the same token type in the output
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:935:    'raiseonerror':   RaiseOnErrorTokenFilter,
venv\Lib\site-packages\pip\_vendor\pygments\filters\__init__.py:938:    'tokenmerge':     TokenMergeFilter,
venv\Lib\site-packages\pip\_vendor\pygments\formatter.py:27:    Converts a token stream to text.
venv\Lib\site-packages\pip\_vendor\pygments\formatter.py:58:        convert the Unicode token strings to byte strings in the
venv\Lib\site-packages\pip\_vendor\pygments\formatter.py:114:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatter.py:116:        This method must format the tokens from the `tokensource` iterable and
venv\Lib\site-packages\pip\_vendor\pygments\formatter.py:119:        Formatter options can control how exactly the tokens are converted.
venv\Lib\site-packages\pip\_vendor\pygments\formatter.py:124:        return self.format_unencoded(tokensource, outfile)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\bbcode.py:20:    Format tokens with BBcodes. These formatting codes are used by many
venv\Lib\site-packages\pip\_vendor\pygments\formatters\bbcode.py:78:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\bbcode.py:87:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\groff.py:20:    Format tokens with groff escapes to change their color and font style.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\groff.py:138:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\groff.py:146:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:18:from pip._vendor.pygments.token import Token, Text, STANDARD_TYPES
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:115:    Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:169:    `get_style_defs()` method to request multiple prefixes for the tokens:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:189:        around the tokens. This disables most other options (default: ``False``).
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:206:        If set to true, token ``<span>`` tags (as well as line number elements)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:212:        Since the token types use relatively short class names, they may clash
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:215:        CSS class names for token types.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:351:    `debug_token_types`
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:352:        Add ``title`` attributes to all token ``<span>`` tags that show the
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:353:        name of the token.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:427:        self.debug_token_types = get_bool_opt(options, 'debug_token_types', False)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:461:        """Return the css class of this token type prefixed with
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:469:        """Return the CSS classes of this token type prefixed with the classprefix option."""
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:477:        """Return the inline CSS styles for this token type."""
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:485:        t2c = self.ttype2class = {Token: ''}
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:512:        insert before the token type classes.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:518:        style_lines.extend(self.get_token_style_defs(arg))
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:522:    def get_token_style_defs(self, arg=None):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:829:    def _format_lines(self, tokensource):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:831:        Just format the tokens, without any wrapping tags.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:840:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:844:                title = ' title="{}"'.format('.'.join(ttype)) if self.debug_token_types else ''
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:862:            if tagsfile and ttype in Token.Name:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:907:    def _lookup_ctag(self, token):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:909:        if self._ctags.find(entry, token.encode(), 0):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:914:    def _highlight_lines(self, tokensource):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:917:        post-processing the token stream coming from `_format_lines`.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:921:        for i, (t, value) in enumerate(tokensource):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:950:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:959:        is part of the original tokensource being highlighted, if it's
venv\Lib\site-packages\pip\_vendor\pygments\formatters\html.py:964:        source = self._format_lines(tokensource)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:487:        Get the correct color for the token from the style.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:497:        Get the correct background color for the token from the style.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:537:    def _create_drawables(self, tokensource):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:539:        Create drawables for the token content.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:543:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:605:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:607:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:610:        This implementation calculates where it should draw each token on the
venv\Lib\site-packages\pip\_vendor\pygments\formatters\img.py:613:        self._create_drawables(tokensource)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:12:from pip._vendor.pygments.token import Keyword, Name, Comment, String, Error, \
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:13:    Number, Operator, Generic, Token, Whitespace
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:20:#: Map token types to a tuple of color values for light and dark
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:23:    Token:              ('',            ''),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:99:    Format tokens with IRC color sequences
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:111:        A dictionary mapping token types to (lightbg, darkbg) color names or
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:135:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\irc.py:138:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:15:from pip._vendor.pygments.token import Token, STANDARD_TYPES
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:63:# each token type defined in the current style.  That obviously is
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:69:# specific token type, thus falling back to the parent token type if one
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:71:# forms given in token.STANDARD_TYPES.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:149:    Format tokens as LaTeX code. This needs the `fancyvrb` and `color`
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:177:        If set to ``True``, don't wrap the tokens at all, not even inside a
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:224:        in comment tokens is not escaped so that LaTeX can render it (default:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:280:        t2n = self.ttype2name = {Token: ''}
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:333:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:356:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:357:            if ttype in Token.Comment:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:395:            elif ttype not in Token.Escape:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:398:            while ttype is not Token:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:443:    strings and comments. All other consecutive tokens are merged and
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:445:    the Token.Escape type. Finally text that is not escaped is scanned
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:454:    def get_tokens_unprocessed(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:455:        # find and remove all the escape tokens (replace with an empty string)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:456:        # this is very similar to DelegatingLexer.get_tokens_unprocessed.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:460:        for i, t, v in self._find_safe_escape_tokens(text):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:471:                             self.lang.get_tokens_unprocessed(buffered))
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:473:    def _find_safe_escape_tokens(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:474:        """ find escape tokens that are not in strings or comments """
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:476:            self.lang.get_tokens_unprocessed(text),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:477:            lambda t: t in Token.Comment or t in Token.String
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:480:                for i2, t2, v2 in self._find_escape_tokens(v):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:486:        """ Keep only the tokens that match `pred`, merge the others together """
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:502:    def _find_escape_tokens(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:503:        """ Find escape tokens within text, give token=None otherwise """
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:513:                    yield index + len(sep1), Token.Escape, b
venv\Lib\site-packages\pip\_vendor\pygments\formatters\latex.py:516:                    yield index, Token.Error, sep1
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:5:    Other formatters: NullFormatter, RawTokenFormatter.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:13:from pip._vendor.pygments.token import Token
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:16:__all__ = ['NullFormatter', 'RawTokenFormatter', 'TestcaseFormatter']
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:27:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:29:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:36:class RawTokenFormatter(Formatter):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:38:    Format tokens as a raw representation for storing token streams.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:40:    The format is ``tokentype<TAB>repr(tokenstring)\n``. The output can later
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:41:    be converted to a token stream with the `RawTokenLexer`, described in the
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:50:        If set to a color name, highlight error tokens using that color.  If
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:56:    name = 'Raw tokens'
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:57:    aliases = ['raw', 'tokens']
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:66:        # The RawTokenFormatter outputs only ASCII. Override here.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:79:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:83:            raise TypeError('The raw tokens formatter needs a binary '
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:106:            for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:108:                if ttype is Token.Error:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:113:            for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:121:        tokens = [
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:125:        assert list(lexer.get_tokens(fragment)) == tokens
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:131:    Format tokens as appropriate for a new testcase.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:143:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\other.py:147:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\pangomarkup.py:30:    Format tokens as Pango Markup code. It can then be rendered to an SVG.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\pangomarkup.py:44:        for token, style in self.style:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\pangomarkup.py:59:            self.styles[token] = (start, end)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\pangomarkup.py:61:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\pangomarkup.py:67:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:22:    Format tokens as RTF markup. This formatter automatically outputs full RTF
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:195:    def _split_tokens_on_newlines(self, tokensource):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:197:        Split tokens containing newline characters into multiple token
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:201:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:278:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:282:        tokensource = self._split_tokens_on_newlines(tokensource)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:284:        # first pass of tokens to count lines, needed for line numbering
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:287:            tokens = [] # for copying the token source generator
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:288:            for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:289:                tokens.append((ttype, value))
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:296:            tokensource = tokens
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:301:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:313:            while not self.style.styles_token(ttype) and ttype.parent:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\rtf.py:315:            style = self.style.style_for_token(ttype)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:12:from pip._vendor.pygments.token import Comment
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:31:    Format tokens as an SVG graphics file.  This formatter is still experimental.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:33:    coordinates containing ``<tspan>`` elements with the individual token styles.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:115:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:117:        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:147:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:170:    def _get_style(self, tokentype):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:171:        if tokentype in self._stylecache:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:172:            return self._stylecache[tokentype]
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:173:        otokentype = tokentype
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:174:        while not self.style.styles_token(tokentype):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:175:            tokentype = tokentype.parent
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:176:        value = self.style.style_for_token(tokentype)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\svg.py:184:        self._stylecache[otokentype] = result
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:12:from pip._vendor.pygments.token import Keyword, Name, Comment, String, Error, \
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:13:    Number, Operator, Generic, Token, Whitespace
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:21:#: Map token types to a tuple of color values for light and dark
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:24:    Token:              ('',            ''),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:58:    Format tokens with ANSI color sequences, for output in a text console.
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:72:        A dictionary mapping token types to (lightbg, darkbg) color names or
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:91:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:92:        return Formatter.format(self, tokensource, outfile)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:100:        # have to walk the tree of dots.  The base Token type must be a key,
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:108:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal.py:112:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal256.py:100:    Format tokens with ANSI color sequences, for output in a 256-color
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal256.py:249:    def format(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal256.py:250:        return Formatter.format(self, tokensource, outfile)
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal256.py:252:    def format_unencoded(self, tokensource, outfile):
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal256.py:256:        for ttype, value in tokensource:
venv\Lib\site-packages\pip\_vendor\pygments\formatters\terminal256.py:295:    Format tokens with ANSI color sequences, for output in a true-color
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:5:    'BBCodeFormatter': ('pygments.formatters.bbcode', 'BBCode', ('bbcode', 'bb'), (), 'Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:8:    'GroffFormatter': ('pygments.formatters.groff', 'groff', ('groff', 'troff', 'roff'), (), 'Format tokens with groff escapes to change their color and font style.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:9:    'HtmlFormatter': ('pygments.formatters.html', 'HTML', ('html',), ('*.html', '*.htm'), "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option."),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:10:    'IRCFormatter': ('pygments.formatters.irc', 'IRC', ('irc', 'IRC'), (), 'Format tokens with IRC color sequences'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:13:    'LatexFormatter': ('pygments.formatters.latex', 'LaTeX', ('latex', 'tex'), ('*.tex',), 'Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:15:    'PangoMarkupFormatter': ('pygments.formatters.pangomarkup', 'Pango Markup', ('pango', 'pangomarkup'), (), 'Format tokens as Pango Markup code. It can then be rendered to an SVG.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:16:    'RawTokenFormatter': ('pygments.formatters.other', 'Raw tokens', ('raw', 'tokens'), ('*.raw',), 'Format tokens as a raw representation for storing token streams.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:17:    'RtfFormatter': ('pygments.formatters.rtf', 'RTF', ('rtf',), ('*.rtf',), 'Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:18:    'SvgFormatter': ('pygments.formatters.svg', 'SVG', ('svg',), ('*.svg',), 'Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:19:    'Terminal256Formatter': ('pygments.formatters.terminal256', 'Terminal256', ('terminal256', 'console256', '256'), (), 'Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:20:    'TerminalFormatter': ('pygments.formatters.terminal', 'Terminal', ('terminal', 'console'), (), 'Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:21:    'TerminalTrueColorFormatter': ('pygments.formatters.terminal256', 'TerminalTrueColor', ('terminal16m', 'console16m', '16m'), (), 'Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.'),
venv\Lib\site-packages\pip\_vendor\pygments\formatters\_mapping.py:22:    'TestcaseFormatter': ('pygments.formatters.other', 'Testcase', ('testcase',), (), 'Format tokens as appropriate for a new testcase.'),
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:17:from pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:255:    def get_tokens(self, text, unfiltered=False):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:259:        iterable of ``(tokentype, value)`` pairs from `text`.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:263:        (`stripnl`, `stripall` and so on), and then yields all tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:264:        from `get_tokens_unprocessed()`, with the ``index`` dropped.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:272:            for _, t, v in self.get_tokens_unprocessed(text):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:279:    def get_tokens_unprocessed(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:282:        ``(index, tokentype, value)`` tuples where ``index`` is the starting
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:283:        position of the token within the input text.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:295:    lexer, afterwards all ``Other`` tokens are lexed using the root
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:307:    def get_tokens_unprocessed(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:311:        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:322:                             self.root_lexer.get_tokens_unprocessed(buffered))
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:395:            elif type(action) is _TokenType:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:454:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:465:            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:475:    For example default('#pop') is equivalent to ('', Token, '#pop')
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:502:    Metaclass for RegexLexer, creates the self._tokens attribute from
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:503:    self.tokens on the first instantiation.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:507:        """Preprocess the regular expression component of a token definition."""
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:512:    def _process_token(cls, token):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:513:        """Preprocess the token component of a token definition."""
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:514:        assert type(token) is _TokenType or callable(token), \
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:515:            f'token type must be simple type or callable, not {token!r}'
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:516:        return token
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:519:        """Preprocess the state transition action of a token definition."""
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:536:            itokens = []
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:539:                itokens.extend(cls._process_state(unprocessed,
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:541:            processed[tmp_state] = itokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:559:        tokens = processed[state] = []
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:565:                tokens.extend(cls._process_state(unprocessed, processed,
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:575:                tokens.append((re.compile('').match, None, new_state))
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:585:            token = cls._process_token(tdef[1])
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:593:            tokens.append((rex, token, new_state))
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:594:        return tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:596:    def process_tokendef(cls, name, tokendefs=None):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:597:        """Preprocess a dictionary of token definitions."""
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:598:        processed = cls._all_tokens[name] = {}
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:599:        tokendefs = tokendefs or cls.tokens[name]
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:600:        for state in list(tokendefs):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:601:            cls._process_state(tokendefs, processed, state)
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:604:    def get_tokendefs(cls):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:606:        Merge tokens from superclasses in MRO order, returning a single tokendef
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:616:        tokens = {}
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:619:            toks = c.__dict__.get('tokens', {})
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:622:                curitems = tokens.get(state)
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:628:                    tokens[state] = items
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:651:        return tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:654:        """Instantiate cls after preprocessing its token definitions."""
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:655:        if '_tokens' not in cls.__dict__:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:656:            cls._all_tokens = {}
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:658:            if hasattr(cls, 'token_variants') and cls.token_variants:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:662:                cls._tokens = cls.process_tokendef('', cls.get_tokendefs())
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:681:    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:700:    tokens = {}
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:702:    def get_tokens_unprocessed(self, text, stack=('root',)):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:704:        Split ``text`` into (tokentype, text) pairs.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:709:        tokendefs = self._tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:711:        statetokens = tokendefs[statestack[-1]]
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:713:            for rexmatch, action, new_state in statetokens:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:717:                        if type(action) is _TokenType:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:745:                        statetokens = tokendefs[statestack[-1]]
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:748:                # We are here only if all state tokens have been considered
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:754:                        statetokens = tokendefs['root']
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:784:    def get_tokens_unprocessed(self, text=None, context=None):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:786:        Split ``text`` into (tokentype, text) pairs.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:789:        tokendefs = self._tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:792:            statetokens = tokendefs['root']
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:795:            statetokens = tokendefs[ctx.stack[-1]]
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:798:            for rexmatch, action, new_state in statetokens:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:802:                        if type(action) is _TokenType:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:809:                                statetokens = tokendefs[ctx.stack[-1]]
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:832:                        statetokens = tokendefs[ctx.stack[-1]]
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:841:                        statetokens = tokendefs['root']
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:851:def do_insertions(insertions, tokens):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:856:    ``insertions`` is a list of ``(index, itokens)`` pairs.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:857:    Each ``itokens`` iterable should be inserted at position
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:858:    ``index`` into the token stream given by the ``tokens``
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:861:    The result is a combined token stream.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:867:        index, itokens = next(insertions)
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:870:        yield from tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:876:    # iterate over the token stream where we want to insert
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:877:    # the tokens from the insertion list.
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:878:    for i, t, v in tokens:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:888:            for it_index, it_token, it_value in itokens:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:889:                yield realpos, it_token, it_value
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:893:                index, itokens = next(insertions)
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:901:    # leftover tokens
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:903:        # no normal tokens, set realpos to zero
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:905:        for p, t, v in itokens:
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:909:            index, itokens = next(insertions)
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:943:    def get_tokens_unprocessed(self, text, stack=('root',)):
venv\Lib\site-packages\pip\_vendor\pygments\lexer.py:946:        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:16:from pip._vendor.pygments.token import Text, Comment, Operator, Keyword, Name, String, \
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:99:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:446:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:645:    Code tokens are output as ``Token.Other.Code``, traceback tokens as
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:646:    ``Token.Other.Traceback``.
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:648:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:713:        # different tokens.  TODO: DelegatingLexer should support this
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:715:        # distinguishing tokens. Then we wouldn't need this intermediary
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:738:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:795:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:839:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:1019:    tokens = {
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:1186:    def get_tokens_unprocessed(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:1187:        for index, token, value in \
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:1188:                PythonLexer.get_tokens_unprocessed(self, text):
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:1189:            if token is Name and value in self.EXTRA_KEYWORDS:
venv\Lib\site-packages\pip\_vendor\pygments\lexers\python.py:1192:                yield index, token, value
venv\Lib\site-packages\pip\_vendor\pygments\lexers\_mapping.py:430:    'RawTokenLexer': ('pip._vendor.pygments.lexers.special', 'Raw token data', (), (), ('application/x-pygments-tokens',)),
venv\Lib\site-packages\pip\_vendor\pygments\style.py:11:from pip._vendor.pygments.token import Token, STANDARD_TYPES
venv\Lib\site-packages\pip\_vendor\pygments\style.py:62:        for token in STANDARD_TYPES:
venv\Lib\site-packages\pip\_vendor\pygments\style.py:63:            if token not in obj.styles:
venv\Lib\site-packages\pip\_vendor\pygments\style.py:64:                obj.styles[token] = ''
venv\Lib\site-packages\pip\_vendor\pygments\style.py:84:            for token in ttype.split():
venv\Lib\site-packages\pip\_vendor\pygments\style.py:85:                if token in _styles:
venv\Lib\site-packages\pip\_vendor\pygments\style.py:87:                ndef = _styles.get(token.parent, None)
venv\Lib\site-packages\pip\_vendor\pygments\style.py:88:                styledefs = obj.styles.get(token, '').split()
venv\Lib\site-packages\pip\_vendor\pygments\style.py:89:                if not ndef or token is None:
venv\Lib\site-packages\pip\_vendor\pygments\style.py:91:                elif 'noinherit' in styledefs and token is not Token:
venv\Lib\site-packages\pip\_vendor\pygments\style.py:92:                    ndef = _styles[Token][:]
venv\Lib\site-packages\pip\_vendor\pygments\style.py:95:                _styles[token] = ndef
venv\Lib\site-packages\pip\_vendor\pygments\style.py:96:                for styledef in obj.styles.get(token, '').split():
venv\Lib\site-packages\pip\_vendor\pygments\style.py:126:    def style_for_token(cls, token):
venv\Lib\site-packages\pip\_vendor\pygments\style.py:127:        t = cls._styles[token]
venv\Lib\site-packages\pip\_vendor\pygments\style.py:159:    def styles_token(cls, ttype):
venv\Lib\site-packages\pip\_vendor\pygments\style.py:163:        for token in cls._styles:
venv\Lib\site-packages\pip\_vendor\pygments\style.py:164:            yield token, cls.style_for_token(token)
venv\Lib\site-packages\pip\_vendor\pygments\style.py:190:    #: Style definitions for individual token types.
venv\Lib\site-packages\pip\_vendor\pygments\token.py:2:    pygments.token
venv\Lib\site-packages\pip\_vendor\pygments\token.py:5:    Basic token types and the standard tokens.
venv\Lib\site-packages\pip\_vendor\pygments\token.py:12:class _TokenType(tuple):
venv\Lib\site-packages\pip\_vendor\pygments\token.py:37:        new = _TokenType(self + (val,))
venv\Lib\site-packages\pip\_vendor\pygments\token.py:44:        return 'Token' + (self and '.' or '') + '.'.join(self)
venv\Lib\site-packages\pip\_vendor\pygments\token.py:55:Token = _TokenType()
venv\Lib\site-packages\pip\_vendor\pygments\token.py:57:# Special token types
venv\Lib\site-packages\pip\_vendor\pygments\token.py:58:Text = Token.Text
venv\Lib\site-packages\pip\_vendor\pygments\token.py:60:Escape = Token.Escape
venv\Lib\site-packages\pip\_vendor\pygments\token.py:61:Error = Token.Error
venv\Lib\site-packages\pip\_vendor\pygments\token.py:63:Other = Token.Other
venv\Lib\site-packages\pip\_vendor\pygments\token.py:65:# Common token types for source code
venv\Lib\site-packages\pip\_vendor\pygments\token.py:66:Keyword = Token.Keyword
venv\Lib\site-packages\pip\_vendor\pygments\token.py:67:Name = Token.Name
venv\Lib\site-packages\pip\_vendor\pygments\token.py:68:Literal = Token.Literal
venv\Lib\site-packages\pip\_vendor\pygments\token.py:71:Punctuation = Token.Punctuation
venv\Lib\site-packages\pip\_vendor\pygments\token.py:72:Operator = Token.Operator
venv\Lib\site-packages\pip\_vendor\pygments\token.py:73:Comment = Token.Comment
venv\Lib\site-packages\pip\_vendor\pygments\token.py:76:Generic = Token.Generic
venv\Lib\site-packages\pip\_vendor\pygments\token.py:78:# String and some others are not direct children of Token.
venv\Lib\site-packages\pip\_vendor\pygments\token.py:80:Token.Token = Token
venv\Lib\site-packages\pip\_vendor\pygments\token.py:81:Token.String = String
venv\Lib\site-packages\pip\_vendor\pygments\token.py:82:Token.Number = Number
venv\Lib\site-packages\pip\_vendor\pygments\token.py:85:def is_token_subtype(ttype, other):
venv\Lib\site-packages\pip\_vendor\pygments\token.py:94:def string_to_tokentype(s):
venv\Lib\site-packages\pip\_vendor\pygments\token.py:96:    Convert a string into a token type::
venv\Lib\site-packages\pip\_vendor\pygments\token.py:98:        >>> string_to_token('String.Double')
venv\Lib\site-packages\pip\_vendor\pygments\token.py:99:        Token.Literal.String.Double
venv\Lib\site-packages\pip\_vendor\pygments\token.py:100:        >>> string_to_token('Token.Literal.Number')
venv\Lib\site-packages\pip\_vendor\pygments\token.py:101:        Token.Literal.Number
venv\Lib\site-packages\pip\_vendor\pygments\token.py:102:        >>> string_to_token('')
venv\Lib\site-packages\pip\_vendor\pygments\token.py:103:        Token
venv\Lib\site-packages\pip\_vendor\pygments\token.py:105:    Tokens that are already tokens are returned unchanged:
venv\Lib\site-packages\pip\_vendor\pygments\token.py:107:        >>> string_to_token(String)
venv\Lib\site-packages\pip\_vendor\pygments\token.py:108:        Token.Literal.String
venv\Lib\site-packages\pip\_vendor\pygments\token.py:110:    if isinstance(s, _TokenType):
venv\Lib\site-packages\pip\_vendor\pygments\token.py:113:        return Token
venv\Lib\site-packages\pip\_vendor\pygments\token.py:114:    node = Token
venv\Lib\site-packages\pip\_vendor\pygments\token.py:120:# Map standard token types to short names, used in CSS class naming.
venv\Lib\site-packages\pip\_vendor\pygments\token.py:124:    Token:                         '',
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:38:    and return an iterable of tokens. Currently, this only calls
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:39:    `lexer.get_tokens()`.
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:42:        return lexer.get_tokens(code)
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:52:def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:54:    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:64:            formatter.format(tokens, realoutfile)
venv\Lib\site-packages\pip\_vendor\pygments\__init__.py:67:            formatter.format(tokens, outfile)
venv\Lib\site-packages\pip\_vendor\requests\adapters.py:281:            username, password = get_auth_from_url(proxy)
venv\Lib\site-packages\pip\_vendor\requests\adapters.py:285:                password=password,
venv\Lib\site-packages\pip\_vendor\requests\adapters.py:606:        username, password = get_auth_from_url(proxy)
venv\Lib\site-packages\pip\_vendor\requests\adapters.py:609:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
venv\Lib\site-packages\pip\_vendor\requests\auth.py:25:def _basic_auth_str(username, password):
venv\Lib\site-packages\pip\_vendor\requests\auth.py:45:    if not isinstance(password, basestring):
venv\Lib\site-packages\pip\_vendor\requests\auth.py:47:            "Non-string passwords will no longer be supported in Requests "
venv\Lib\site-packages\pip\_vendor\requests\auth.py:50:            "problems.".format(type(password)),
venv\Lib\site-packages\pip\_vendor\requests\auth.py:53:        password = str(password)
venv\Lib\site-packages\pip\_vendor\requests\auth.py:59:    if isinstance(password, str):
venv\Lib\site-packages\pip\_vendor\requests\auth.py:60:        password = password.encode("latin1")
venv\Lib\site-packages\pip\_vendor\requests\auth.py:63:        b64encode(b":".join((username, password))).strip()
venv\Lib\site-packages\pip\_vendor\requests\auth.py:79:    def __init__(self, username, password):
venv\Lib\site-packages\pip\_vendor\requests\auth.py:81:        self.password = password
venv\Lib\site-packages\pip\_vendor\requests\auth.py:87:                self.password == getattr(other, "password", None),
venv\Lib\site-packages\pip\_vendor\requests\auth.py:95:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
venv\Lib\site-packages\pip\_vendor\requests\auth.py:103:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
venv\Lib\site-packages\pip\_vendor\requests\auth.py:110:    def __init__(self, username, password):
venv\Lib\site-packages\pip\_vendor\requests\auth.py:112:        self.password = password
venv\Lib\site-packages\pip\_vendor\requests\auth.py:189:        A1 = f"{self.username}:{realm}:{self.password}"
venv\Lib\site-packages\pip\_vendor\requests\auth.py:309:                self.password == getattr(other, "password", None),
venv\Lib\site-packages\pip\_vendor\requests\sessions.py:322:            username, password = get_auth_from_url(new_proxies[scheme])
venv\Lib\site-packages\pip\_vendor\requests\sessions.py:324:            username, password = None, None
venv\Lib\site-packages\pip\_vendor\requests\sessions.py:328:        if not scheme.startswith("https") and username and password:
venv\Lib\site-packages\pip\_vendor\requests\sessions.py:329:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
venv\Lib\site-packages\pip\_vendor\requests\utils.py:247:                # Return with login / password
venv\Lib\site-packages\pip\_vendor\requests\utils.py:388:    >>> parse_list_header('token, "quoted value"')
venv\Lib\site-packages\pip\_vendor\requests\utils.py:389:    ['token', 'quoted value']
venv\Lib\site-packages\pip\_vendor\requests\utils.py:522:    tokens = header.split(";")
venv\Lib\site-packages\pip\_vendor\requests\utils.py:523:    content_type, params = tokens[0].strip(), tokens[1:]
venv\Lib\site-packages\pip\_vendor\requests\utils.py:1020:    username,password.
venv\Lib\site-packages\pip\_vendor\requests\utils.py:1027:        auth = (unquote(parsed.username), unquote(parsed.password))
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:20:class _AnsiToken(NamedTuple):
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:21:    """Result of ansi tokenized string."""
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:28:def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:29:    """Tokenize a string in to plain text and ANSI codes.
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:35:        AnsiToken: A named tuple of (plain, sgr, osc)
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:45:            yield _AnsiToken(ansi_text[position:start])
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:51:                yield _AnsiToken("", sgr[1:-1], osc)
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:53:            yield _AnsiToken("", sgr, osc)
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:56:        yield _AnsiToken(ansi_text[position:])
venv\Lib\site-packages\pip\_vendor\rich\ansi.py:153:        for plain_text, sgr, osc in _ansi_tokenize(line):
venv\Lib\site-packages\pip\_vendor\rich\console.py:2126:        password: bool = False,
venv\Lib\site-packages\pip\_vendor\rich\console.py:2137:            password: (bool, optional): Hide typed text. Defaults to False.
venv\Lib\site-packages\pip\_vendor\rich\console.py:2145:        if password:
venv\Lib\site-packages\pip\_vendor\rich\containers.py:157:                tokens: List[Text] = []
venv\Lib\site-packages\pip\_vendor\rich\containers.py:161:                    tokens.append(word)
venv\Lib\site-packages\pip\_vendor\rich\containers.py:166:                        tokens.append(Text(" " * spaces[index], style=space_style))
venv\Lib\site-packages\pip\_vendor\rich\containers.py:167:                self[line_index] = Text("").join(tokens)
venv\Lib\site-packages\pip\_vendor\rich\logging.py:281:    log.warning("password was rejected for admin site.")
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:424:    def iter_tokens(self) -> Iterable[str]:
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:425:        """Generate tokens for this node."""
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:435:                    yield from self.children[0].iter_tokens()
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:439:                        yield from child.iter_tokens()
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:457:        for token in self.iter_tokens():
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:458:            total_length += cell_len(token)
venv\Lib\site-packages\pip\_vendor\rich\pretty.py:464:        repr_text = "".join(self.iter_tokens())
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:37:        password (bool, optional): Enable password input. Defaults to False.
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:59:        password: bool = False,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:71:        self.password = password
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:85:        password: bool = False,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:102:        password: bool = False,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:117:        password: bool = False,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:133:            password (bool, optional): Enable password input. Defaults to False.
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:143:            password=password,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:198:        password: bool,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:206:            password (bool): Enable password entry.
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:211:        return console.input(prompt, password=password, stream=stream)
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:292:            value = self.get_input(self.console, prompt, self.password, stream=stream)
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:380:            password = Prompt.ask(
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:381:                "Please enter a password [cyan](must be at least 5 characters)",
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:382:                password=True,
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:384:            if len(password) >= 5:
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:386:            print("[prompt.invalid]password too short")
venv\Lib\site-packages\pip\_vendor\rich\prompt.py:387:        print(f"password={password!r}")
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:25:from pip._vendor.pygments.token import (
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:34:    Token,
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:52:TokenType = Tuple[str, ...]
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:60:ANSI_LIGHT: Dict[TokenType, Style] = {
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:61:    Token: Style(),
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:89:ANSI_DARK: Dict[TokenType, Style] = {
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:90:    Token: Style(),
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:126:    def get_style_for_token(self, token_type: TokenType) -> Style:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:127:        """Get a style for a given Pygments token."""
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:140:        self._style_cache: Dict[TokenType, Style] = {}
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:152:    def get_style_for_token(self, token_type: TokenType) -> Style:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:155:            return self._style_cache[token_type]
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:158:                pygments_style = self._pygments_style_class.style_for_token(token_type)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:171:            self._style_cache[token_type] = style
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:181:    def __init__(self, style_map: Dict[TokenType, Style]) -> None:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:185:        self._style_cache: Dict[TokenType, Style] = {}
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:187:    def get_style_for_token(self, token_type: TokenType) -> Style:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:190:            return self._style_cache[token_type]
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:196:            token = tuple(token_type)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:198:            while token:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:199:                _style = get_style(token)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:203:                token = token[:-1]
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:204:            self._style_cache[token_type] = style
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:411:    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:412:        """Get a color (if any) for the given token.
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:415:            token_type (TokenType): A token type tuple from Pygments.
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:420:        style = self._theme.get_style_for_token(token_type)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:478:        _get_theme_style = self._theme.get_style_for_token
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:490:                def line_tokenize() -> Iterable[Tuple[Any, str]]:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:491:                    """Split tokens to one per line."""
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:494:                    for token_type, token in lexer.get_tokens(code):
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:495:                        while token:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:496:                            line_token, new_line, token = token.partition("\n")
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:497:                            yield token_type, line_token + new_line
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:499:                def tokens_to_spans() -> Iterable[Tuple[str, Optional[Style]]]:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:500:                    """Convert tokens to spans."""
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:501:                    tokens = iter(line_tokenize())
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:505:                    # Skip over tokens until line start
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:508:                            _token_type, token = next(tokens)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:511:                        yield (token, None)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:512:                        if token.endswith("\n"):
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:515:                    for token_type, token in tokens:
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:516:                        yield (token, _get_theme_style(token_type))
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:517:                        if token.endswith("\n"):
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:522:                text.append_tokens(tokens_to_spans())
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:525:                text.append_tokens(
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:526:                    (token, _get_theme_style(token_type))
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:527:                    for token_type, token in lexer.get_tokens(code)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:563:        foreground_color = self._get_token_color(Token.Text)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:592:                self._theme.get_style_for_token(Token.Text),
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:598:                self._theme.get_style_for_token(Token.Text),
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:662:                + self._theme.get_style_for_token(Comment)
venv\Lib\site-packages\pip\_vendor\rich\syntax.py:698:                + self._theme.get_style_for_token(Comment)
venv\Lib\site-packages\pip\_vendor\rich\text.py:1031:    def append_tokens(
venv\Lib\site-packages\pip\_vendor\rich\text.py:1032:        self, tokens: Iterable[Tuple[str, Optional[StyleType]]]
venv\Lib\site-packages\pip\_vendor\rich\text.py:1037:            tokens (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.
venv\Lib\site-packages\pip\_vendor\rich\text.py:1046:        for content, style in tokens:
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:23:from pip._vendor.pygments.token import Comment, Keyword, Name, Number, Operator, String
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:24:from pip._vendor.pygments.token import Text as TextToken
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:25:from pip._vendor.pygments.token import Token
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:534:        token_style = theme.get_style_for_token
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:538:                "pretty": token_style(TextToken),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:539:                "pygments.text": token_style(Token),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:540:                "pygments.string": token_style(String),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:541:                "pygments.function": token_style(Name.Function),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:542:                "pygments.number": token_style(Number),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:543:                "repr.indent": token_style(Comment) + Style(dim=True),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:544:                "repr.str": token_style(String),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:545:                "repr.brace": token_style(TextToken) + Style(bold=True),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:546:                "repr.number": token_style(Number),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:547:                "repr.bool_true": token_style(Keyword.Constant),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:548:                "repr.bool_false": token_style(Keyword.Constant),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:549:                "repr.none": token_style(Keyword.Constant),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:550:                "scope.border": token_style(String.Delimiter),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:551:                "scope.equals": token_style(Operator),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:552:                "scope.key": token_style(Name),
venv\Lib\site-packages\pip\_vendor\rich\traceback.py:553:                "scope.key.special": token_style(Name.Constant) + Style(dim=True),
venv\Lib\site-packages\pip\_vendor\rich\_emoji_codes.py:156:    "japanese_secret_button": "㊙",
venv\Lib\site-packages\pip\_vendor\rich\_emoji_codes.py:2882:    "secret": "㊙",
venv\Lib\site-packages\pip\_vendor\truststore\_api.py:29:_PasswordType: typing.TypeAlias = str | bytes | typing.Callable[[], str | bytes]
venv\Lib\site-packages\pip\_vendor\truststore\_api.py:143:        password: _PasswordType | None = None,
venv\Lib\site-packages\pip\_vendor\truststore\_api.py:146:            certfile=certfile, keyfile=keyfile, password=password
venv\Lib\site-packages\pip\_vendor\urllib3\connection.py:215:                "Method cannot contain non-token characters %r (found at least %r)"
venv\Lib\site-packages\pip\_vendor\urllib3\connection.py:308:        key_password=None,
venv\Lib\site-packages\pip\_vendor\urllib3\connection.py:320:        self.key_password = key_password
venv\Lib\site-packages\pip\_vendor\urllib3\connection.py:333:        key_password=None,
venv\Lib\site-packages\pip\_vendor\urllib3\connection.py:354:        self.key_password = key_password
venv\Lib\site-packages\pip\_vendor\urllib3\connection.py:423:            key_password=self.key_password,
venv\Lib\site-packages\pip\_vendor\urllib3\connectionpool.py:925:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
venv\Lib\site-packages\pip\_vendor\urllib3\connectionpool.py:948:        key_password=None,
venv\Lib\site-packages\pip\_vendor\urllib3\connectionpool.py:975:        self.key_password = key_password
venv\Lib\site-packages\pip\_vendor\urllib3\connectionpool.py:991:                key_password=self.key_password,
venv\Lib\site-packages\pip\_vendor\urllib3\connectionpool.py:1047:            key_password=self.key_password,
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\ntlmpool.py:38:        pw is the password for the user.
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\ntlmpool.py:108:                raise Exception("Server rejected request: wrong username or password")
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\pyopenssl.py:473:    def load_cert_chain(self, certfile, keyfile=None, password=None):
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\pyopenssl.py:475:        if password is not None:
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\pyopenssl.py:476:            if not isinstance(password, six.binary_type):
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\pyopenssl.py:477:                password = password.encode("utf-8")
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\pyopenssl.py:478:            self._ctx.set_passwd_cb(lambda *_: password)
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\securetransport.py:872:    def load_cert_chain(self, certfile, keyfile=None, password=None):
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\securetransport.py:875:        self._client_cert_passphrase = password
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:15:- Usernames and passwords for the SOCKS proxy
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:32:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:33:of the ``proxy_url`` will be sent as the username/password to authenticate
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:38:    proxy_url="socks5h://<username>:<password>@proxy-host"
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:102:                proxy_password=self._socks_options["password"],
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:174:        password=None,
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:181:        if username is None and password is None and parsed.auth is not None:
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:184:                username, password = split
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\socks.py:207:            "password": password,
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\_securetransport\low_level.py:215:    credentials. This keychain uses a one-time password and a temporary file to
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\_securetransport\low_level.py:227:    # some random bytes to password-protect the keychain we're creating, so we
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\_securetransport\low_level.py:231:    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
venv\Lib\site-packages\pip\_vendor\urllib3\contrib\_securetransport\low_level.py:239:        keychain_path, len(password), password, False, None, ctypes.byref(keychain)
venv\Lib\site-packages\pip\_vendor\urllib3\packages\six.py:441:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
venv\Lib\site-packages\pip\_vendor\urllib3\packages\six.py:442:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
venv\Lib\site-packages\pip\_vendor\urllib3\poolmanager.py:36:    "key_password",
venv\Lib\site-packages\pip\_vendor\urllib3\poolmanager.py:52:    "key_key_password",  # str
venv\Lib\site-packages\pip\_vendor\urllib3\util\request.py:45:        Colon-separated username:password string for 'authorization: basic ...'
venv\Lib\site-packages\pip\_vendor\urllib3\util\request.py:49:        Colon-separated username:password string for 'proxy-authorization: basic ...'
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:375:    key_password=None,
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:394:    :param key_password:
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:395:        Optional password if the keyfile is encrypted.
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:422:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:423:        raise SSLError("Client private key is encrypted, password is required")
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:426:        if key_password is None:
venv\Lib\site-packages\pip\_vendor\urllib3\util\ssl_.py:429:            context.load_cert_chain(certfile, keyfile, key_password)
venv\Lib\site-packages\pip\_vendor\urllib3\util\url.py:146:            >>> Url('http', 'username:password', 'host.com', 80,
venv\Lib\site-packages\pip\_vendor\urllib3\util\url.py:148:            'http://username:password@host.com:80/path?query#fragment'
venv\Lib\site-packages\pip-25.0.1.dist-info\RECORD:420:pip/_vendor/packaging/__pycache__/_tokenizer.cpython-312.pyc,,
venv\Lib\site-packages\pip-25.0.1.dist-info\RECORD:433:pip/_vendor/packaging/_tokenizer.py,sha256=J6v5H7Jzvb-g81xp_2QACKwO7LxHQA6ikryMU7zXwN8,5273
venv\Lib\site-packages\pip-25.0.1.dist-info\RECORD:480:pip/_vendor/pygments/__pycache__/token.cpython-312.pyc,,
venv\Lib\site-packages\pip-25.0.1.dist-info\RECORD:534:pip/_vendor/pygments/token.py,sha256=qZwT7LSPy5YBY3JgDjut642CCy7JdQzAfmqD9NmT5j0,6226
venv\Lib\site-packages\requests\adapters.py:257:            username, password = get_auth_from_url(proxy)
venv\Lib\site-packages\requests\adapters.py:261:                password=password,
venv\Lib\site-packages\requests\adapters.py:583:        username, password = get_auth_from_url(proxy)
venv\Lib\site-packages\requests\adapters.py:586:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
venv\Lib\site-packages\requests\auth.py:25:def _basic_auth_str(username, password):
venv\Lib\site-packages\requests\auth.py:45:    if not isinstance(password, basestring):
venv\Lib\site-packages\requests\auth.py:47:            "Non-string passwords will no longer be supported in Requests "
venv\Lib\site-packages\requests\auth.py:50:            "problems.".format(type(password)),
venv\Lib\site-packages\requests\auth.py:53:        password = str(password)
venv\Lib\site-packages\requests\auth.py:59:    if isinstance(password, str):
venv\Lib\site-packages\requests\auth.py:60:        password = password.encode("latin1")
venv\Lib\site-packages\requests\auth.py:63:        b64encode(b":".join((username, password))).strip()
venv\Lib\site-packages\requests\auth.py:79:    def __init__(self, username, password):
venv\Lib\site-packages\requests\auth.py:81:        self.password = password
venv\Lib\site-packages\requests\auth.py:87:                self.password == getattr(other, "password", None),
venv\Lib\site-packages\requests\auth.py:95:        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
venv\Lib\site-packages\requests\auth.py:103:        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
venv\Lib\site-packages\requests\auth.py:110:    def __init__(self, username, password):
venv\Lib\site-packages\requests\auth.py:112:        self.password = password
venv\Lib\site-packages\requests\auth.py:189:        A1 = f"{self.username}:{realm}:{self.password}"
venv\Lib\site-packages\requests\auth.py:309:                self.password == getattr(other, "password", None),
venv\Lib\site-packages\requests\sessions.py:322:            username, password = get_auth_from_url(new_proxies[scheme])
venv\Lib\site-packages\requests\sessions.py:324:            username, password = None, None
venv\Lib\site-packages\requests\sessions.py:328:        if not scheme.startswith("https") and username and password:
venv\Lib\site-packages\requests\sessions.py:329:            headers["Proxy-Authorization"] = _basic_auth_str(username, password)
venv\Lib\site-packages\requests\utils.py:237:                # Return with login / password
venv\Lib\site-packages\requests\utils.py:378:    >>> parse_list_header('token, "quoted value"')
venv\Lib\site-packages\requests\utils.py:379:    ['token', 'quoted value']
venv\Lib\site-packages\requests\utils.py:512:    tokens = header.split(";")
venv\Lib\site-packages\requests\utils.py:513:    content_type, params = tokens[0].strip(), tokens[1:]
venv\Lib\site-packages\requests\utils.py:1010:    username,password.
venv\Lib\site-packages\requests\utils.py:1017:        auth = (unquote(parsed.username), unquote(parsed.password))
venv\Lib\site-packages\scikit_learn-1.7.2.dist-info\METADATA:105:.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:8:def generate_random_token():
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:10:    tokens = list(np.arange(k, dtype=int))
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:11:    tokens += list(np.arange(k, dtype=float))
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:12:    tokens += list(string.ascii_letters)
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:13:    tokens += [None for i in range(k)]
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:14:    tokens = np.array(tokens, dtype=object)
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:19:        element = rng.choice(tokens, size)
venv\Lib\site-packages\scipy\cluster\tests\test_disjoint_set.py:29:    for element in generate_random_token():
venv\Lib\site-packages\scipy\fftpack\tests\test_import.py:15:import tokenize
venv\Lib\site-packages\scipy\fftpack\tests\test_import.py:28:            # use tokenize to auto-detect encoding on systems where no
venv\Lib\site-packages\scipy\fftpack\tests\test_import.py:30:            with tokenize.open(str(path)) as file:
venv\Lib\site-packages\scipy\io\arff\_arffread.py:503:def tokenize_attribute(iterable, attribute):
venv\Lib\site-packages\scipy\io\arff\_arffread.py:533:    >>> from scipy.io.arff._arffread import tokenize_attribute
venv\Lib\site-packages\scipy\io\arff\_arffread.py:535:    >>> tokenize_attribute(iterable, r"@attribute floupi real")
venv\Lib\site-packages\scipy\io\arff\_arffread.py:541:    >>> tokenize_attribute(iterable, r"  @attribute 'floupi 2' real   ")
venv\Lib\site-packages\scipy\io\arff\_arffread.py:551:            name, type = tokenize_single_comma(atrv)
venv\Lib\site-packages\scipy\io\arff\_arffread.py:554:            name, type = tokenize_single_wcomma(atrv)
venv\Lib\site-packages\scipy\io\arff\_arffread.py:572:def tokenize_single_comma(val):
venv\Lib\site-packages\scipy\io\arff\_arffread.py:581:            raise ValueError("Error while tokenizing attribute") from e
venv\Lib\site-packages\scipy\io\arff\_arffread.py:583:        raise ValueError(f"Error while tokenizing single {val}")
venv\Lib\site-packages\scipy\io\arff\_arffread.py:587:def tokenize_single_wcomma(val):
venv\Lib\site-packages\scipy\io\arff\_arffread.py:596:            raise ValueError("Error while tokenizing attribute") from e
venv\Lib\site-packages\scipy\io\arff\_arffread.py:598:        raise ValueError(f"Error while tokenizing single {val}")
venv\Lib\site-packages\scipy\io\arff\_arffread.py:613:                attr, i = tokenize_attribute(ofile, i)
venv\Lib\site-packages\scipy\io\arff\_arffread.py:640:                attr, i = tokenize_attribute(ofile, i)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:18:TOKENS = {
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:169:class Token:
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:176:        return f"""Token('{self.type}', "{self.value}")"""
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:182:class Tokenizer:
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:184:        self.tokens = list(TOKENS.keys())
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:185:        self.res = [re.compile(TOKENS[i]) for i in self.tokens]
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:192:    def next_token(self):
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:202:                    return Token(self.tokens[i], m.group(), self.curpos)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:234:        self.tokenizer = threading.local()
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:237:        if not hasattr(self.tokenizer, 't'):
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:238:            self.tokenizer.t = Tokenizer()
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:240:        self.tokenizer.t.input(s)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:242:        tokens = []
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:246:                t = self.tokenizer.t.next_token()
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:250:                    tokens.append(t)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:251:            return self._parse_format(tokens)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:255:    def _get_min(self, tokens):
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:256:        next = tokens.pop(0)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:259:        next = tokens.pop(0)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:262:    def _expect(self, token, tp):
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:263:        if not token.type == tp:
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:266:    def _parse_format(self, tokens):
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:267:        if not tokens[0].type == "LPAR":
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:269:                f"Expected left parenthesis at position {0} (got '{tokens[0].value}')"
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:271:        elif not tokens[-1].type == "RPAR":
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:273:                              f"{len(tokens)} (got '{tokens[-1].value}')")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:275:        tokens = tokens[1:-1]
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:276:        types = [t.type for t in tokens]
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:278:            repeat = int(tokens.pop(0).value)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:282:        next = tokens.pop(0)
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:284:            next = self._next(tokens, "INT")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:286:            if tokens:
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:287:                min = int(self._get_min(tokens))
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:292:            next = self._next(tokens, "INT")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:295:            next = self._next(tokens, "DOT")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:297:            next = self._next(tokens, "INT")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:300:            if tokens:
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:301:                next = self._next(tokens, "EXP_ID")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:303:                next = self._next(tokens, "INT")
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:311:    def _next(self, tokens, tp):
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:312:        if not len(tokens) > 0:
venv\Lib\site-packages\scipy\io\_harwell_boeing\_fortran_format_parser.py:314:        next = tokens.pop(0)
venv\Lib\site-packages\scipy\_lib\tests\test_warnings.py:11:import tokenize
venv\Lib\site-packages\scipy\_lib\tests\test_warnings.py:86:        # use tokenize to auto-detect encoding on systems where no
venv\Lib\site-packages\scipy\_lib\tests\test_warnings.py:88:        with tokenize.open(str(path)) as file:
venv\Lib\site-packages\scipy\_lib\_array_api.py:257:    token = _default_xp_ctxvar.set(xp)
venv\Lib\site-packages\scipy\_lib\_array_api.py:261:        _default_xp_ctxvar.reset(token)
venv\Lib\site-packages\scipy-1.16.3.dist-info\LICENSE.txt:578:source code form), and must require no special password or key for
venv\Lib\site-packages\scipy-1.16.3.dist-info\METADATA:583:         source code form), and must require no special password or key for
venv\Lib\site-packages\six.py:421:    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
venv\Lib\site-packages\six.py:422:    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
venv\Lib\site-packages\sklearn\datasets\descr\twenty_newsgroups.rst:99:  of unigram tokens from a subset of 20news::
venv\Lib\site-packages\sklearn\datasets\descr\twenty_newsgroups.rst:119:  returns ready-to-use token counts features instead of file names.
venv\Lib\site-packages\sklearn\datasets\_rcv1.py:34:# while the original stemmed token files can be found
venv\Lib\site-packages\sklearn\decomposition\_lda.py:331:    >>> # This produces a feature matrix of token counts, similar to what
venv\Lib\site-packages\sklearn\externals\_arff.py:291:    # _RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_feature_hasher.py:153:    # check that some of the hashed tokens are added
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:65:def split_tokenize(s):
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:174:    # with custom tokenizer
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:175:    wa = Vectorizer(tokenizer=split_tokenize, strip_accents="ascii").build_analyzer()
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:392:def test_countvectorizer_custom_token_pattern():
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:393:    """Check `get_feature_names_out()` when a custom token pattern is passed.
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:403:    token_pattern = r"[0-9]{1,3}(?:st|nd|rd|th)\s\b(\w{2,})\b"
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:404:    vectorizer = CountVectorizer(token_pattern=token_pattern)
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:411:def test_countvectorizer_custom_token_pattern_with_several_group():
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:412:    """Check that we raise an error if token pattern capture several groups.
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:423:    token_pattern = r"([0-9]{1,3}(?:st|nd|rd|th))\s\b(\w{2,})\b"
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:424:    err_msg = "More than 1 capturing group in token pattern"
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:425:    vectorizer = CountVectorizer(token_pattern=token_pattern)
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:649:    token_nnz = X.nnz
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:672:    assert ngrams_nnz > token_nnz
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:673:    assert ngrams_nnz < 2 * token_nnz
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1012:    # When norm is None and not alternate_sign, the tokens are counted up to
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1059:        CountVectorizer.build_tokenizer,
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1063:    """Tokenizers cannot be pickled
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1314:    tokenize = estimator.build_tokenizer()
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1316:    return estimator._check_stop_words_consistency(stop_words, preprocess, tokenize)
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1323:        "preprocessing. Tokenizing the stop words generated "
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1324:        "tokens %s not in stop_words." % lstr
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1395:        tokenizer=lambda doc: re.compile(r"\w{1,}").findall(doc), stop_words=["and"]
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1454:        "stop_words, tokenizer, preprocessor, ngram_range, token_pattern,"
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1476:            "'tokenizer'",
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1487:            "'token_pattern'",
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1488:            "'tokenizer'",
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1520:            "'token_pattern'",
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1529:    tokenizer,
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1532:    token_pattern,
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1543:        tokenizer=tokenizer,
venv\Lib\site-packages\sklearn\feature_extraction\tests\test_text.py:1546:        token_pattern=token_pattern,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:71:    tokenizer=None,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:78:    a single document to ngrams, with or without tokenizing or preprocessing.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:81:    intended to replace the preprocessor, tokenizer, and ngrams steps.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:86:    tokenizer: callable, default=None
venv\Lib\site-packages\sklearn\feature_extraction\text.py:95:        A sequence of tokens, possibly with pairs, triples, etc.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:105:        if tokenizer is not None:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:106:            doc = tokenizer(doc)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:202:    """Provides common code for text vectorizers (tokenization logic)."""
venv\Lib\site-packages\sklearn\feature_extraction\text.py:238:    def _word_ngrams(self, tokens, stop_words=None):
venv\Lib\site-packages\sklearn\feature_extraction\text.py:239:        """Turn tokens into a sequence of n-grams after stop words filtering"""
venv\Lib\site-packages\sklearn\feature_extraction\text.py:242:            tokens = [w for w in tokens if w not in stop_words]
venv\Lib\site-packages\sklearn\feature_extraction\text.py:244:        # handle token n-grams
venv\Lib\site-packages\sklearn\feature_extraction\text.py:247:            original_tokens = tokens
venv\Lib\site-packages\sklearn\feature_extraction\text.py:250:                # just iterate through the original tokens
venv\Lib\site-packages\sklearn\feature_extraction\text.py:251:                tokens = list(original_tokens)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:254:                tokens = []
venv\Lib\site-packages\sklearn\feature_extraction\text.py:256:            n_original_tokens = len(original_tokens)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:259:            tokens_append = tokens.append
venv\Lib\site-packages\sklearn\feature_extraction\text.py:262:            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):
venv\Lib\site-packages\sklearn\feature_extraction\text.py:263:                for i in range(n_original_tokens - n + 1):
venv\Lib\site-packages\sklearn\feature_extraction\text.py:264:                    tokens_append(space_join(original_tokens[i : i + n]))
venv\Lib\site-packages\sklearn\feature_extraction\text.py:266:        return tokens
venv\Lib\site-packages\sklearn\feature_extraction\text.py:269:        """Tokenize text_document into a sequence of character n-grams"""
venv\Lib\site-packages\sklearn\feature_extraction\text.py:292:        """Whitespace sensitive char-n-gram tokenization.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:294:        Tokenize text_document into a sequence of character n-grams
venv\Lib\site-packages\sklearn\feature_extraction\text.py:320:        """Return a function to preprocess the text before tokenization.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:325:              A function to preprocess the text before tokenization.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:346:    def build_tokenizer(self):
venv\Lib\site-packages\sklearn\feature_extraction\text.py:347:        """Return a function that splits a string into a sequence of tokens.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:351:        tokenizer: callable
venv\Lib\site-packages\sklearn\feature_extraction\text.py:352:              A function to split a string into a sequence of tokens.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:354:        if self.tokenizer is not None:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:355:            return self.tokenizer
venv\Lib\site-packages\sklearn\feature_extraction\text.py:356:        token_pattern = re.compile(self.token_pattern)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:358:        if token_pattern.groups > 1:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:360:                "More than 1 capturing group in token pattern. Only a single "
venv\Lib\site-packages\sklearn\feature_extraction\text.py:364:        return token_pattern.findall
venv\Lib\site-packages\sklearn\feature_extraction\text.py:376:    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):
venv\Lib\site-packages\sklearn\feature_extraction\text.py:382:                        and tokenizer, False if they are not, None if the check
venv\Lib\site-packages\sklearn\feature_extraction\text.py:385:                        preprocessor / tokenizer)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:395:                tokens = list(tokenize(preprocess(w)))
venv\Lib\site-packages\sklearn\feature_extraction\text.py:396:                for token in tokens:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:397:                    if token not in stop_words:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:398:                        inconsistent.add(token)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:404:                    "your preprocessing. Tokenizing the stop "
venv\Lib\site-packages\sklearn\feature_extraction\text.py:405:                    "words generated tokens %r not in "
venv\Lib\site-packages\sklearn\feature_extraction\text.py:411:            # preprocessor or tokenizer was used)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:418:        The callable handles preprocessing, tokenization, and n-grams generation.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:423:            A function to handle preprocessing, tokenization
venv\Lib\site-packages\sklearn\feature_extraction\text.py:450:            tokenize = self.build_tokenizer()
venv\Lib\site-packages\sklearn\feature_extraction\text.py:451:            self._check_stop_words_consistency(stop_words, preprocess, tokenize)
venv\Lib\site-packages\sklearn\feature_extraction\text.py:455:                tokenizer=tokenize,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:463:                "%s is not a valid tokenization scheme/analyzer" % self.analyzer
venv\Lib\site-packages\sklearn\feature_extraction\text.py:516:        if self.tokenizer is not None and self.token_pattern is not None:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:518:                "The parameter 'token_pattern' will not be used"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:519:                " since 'tokenizer' is not None'"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:544:                self.token_pattern is not None
venv\Lib\site-packages\sklearn\feature_extraction\text.py:545:                and self.token_pattern != r"(?u)\b\w\w+\b"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:548:                    "The parameter 'token_pattern' will not be used"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:551:            if self.tokenizer is not None:
venv\Lib\site-packages\sklearn\feature_extraction\text.py:553:                    "The parameter 'tokenizer' will not be used"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:561:    r"""Convert a collection of text documents to a matrix of token occurrences.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:564:    token occurrence counts (or binary occurrence information), possibly
venv\Lib\site-packages\sklearn\feature_extraction\text.py:565:    normalized as token frequencies if norm='l1' or projected on the euclidean
venv\Lib\site-packages\sklearn\feature_extraction\text.py:569:    token string name to feature integer index mapping.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:589:    - there can be collisions: distinct tokens can be mapped to the same
venv\Lib\site-packages\sklearn\feature_extraction\text.py:641:        Convert all characters to lowercase before tokenizing.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:645:        preserving the tokenizing and n-grams generation steps.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:648:    tokenizer : callable, default=None
venv\Lib\site-packages\sklearn\feature_extraction\text.py:649:        Override the string tokenization step while preserving the
venv\Lib\site-packages\sklearn\feature_extraction\text.py:659:        will be removed from the resulting tokens.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:662:    token_pattern : str or None, default=r"(?u)\\b\\w\\w+\\b"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:663:        Regular expression denoting what constitutes a "token", only used
venv\Lib\site-packages\sklearn\feature_extraction\text.py:664:        if ``analyzer == 'word'``. The default regexp selects tokens of 2
venv\Lib\site-packages\sklearn\feature_extraction\text.py:666:        and always treated as a token separator).
venv\Lib\site-packages\sklearn\feature_extraction\text.py:668:        If there is a capturing group in token_pattern then the
venv\Lib\site-packages\sklearn\feature_extraction\text.py:669:        captured group content, not the entire match, becomes the token.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:719:        token counts.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:752:        "tokenizer": [callable, None],
venv\Lib\site-packages\sklearn\feature_extraction\text.py:754:        "token_pattern": [str, None],
venv\Lib\site-packages\sklearn\feature_extraction\text.py:773:        tokenizer=None,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:775:        token_pattern=r"(?u)\b\w\w+\b",
venv\Lib\site-packages\sklearn\feature_extraction\text.py:789:        self.tokenizer = tokenizer
venv\Lib\site-packages\sklearn\feature_extraction\text.py:792:        self.token_pattern = token_pattern
venv\Lib\site-packages\sklearn\feature_extraction\text.py:863:            constructor argument) which will be tokenized and hashed.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:893:            constructor argument) which will be tokenized and hashed.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:930:    r"""Convert a collection of text documents to a matrix of token counts.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:979:        Convert all characters to lowercase before tokenizing.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:983:        preserving the tokenizing and n-grams generation steps.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:986:    tokenizer : callable, default=None
venv\Lib\site-packages\sklearn\feature_extraction\text.py:987:        Override the string tokenization step while preserving the
venv\Lib\site-packages\sklearn\feature_extraction\text.py:997:        will be removed from the resulting tokens.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1004:    token_pattern : str or None, default=r"(?u)\\b\\w\\w+\\b"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1005:        Regular expression denoting what constitutes a "token", only used
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1006:        if ``analyzer == 'word'``. The default regexp select tokens of 2
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1008:        and always treated as a token separator).
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1010:        If there is a capturing group in token_pattern then the
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1011:        captured group content, not the entire match, becomes the token.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1087:        matrix of token counts.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1136:        "tokenizer": [callable, None],
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1138:        "token_pattern": [str, None],
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1164:        tokenizer=None,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1166:        token_pattern=r"(?u)\b\w\w+\b",
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1181:        self.tokenizer = tokenizer
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1184:        self.token_pattern = token_pattern
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1313:        """Learn a vocabulary dictionary of all tokens in the raw documents.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1402:        Extract token counts out of raw text documents using the vocabulary
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1501:    token in a given document is to scale down the impact of tokens that occur
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1586:        of token occurrences.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1642:            A matrix of term/token counts.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1689:            A matrix of term/token counts.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1788:        Convert all characters to lowercase before tokenizing.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1792:        preserving the tokenizing and n-grams generation steps.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1795:    tokenizer : callable, default=None
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1796:        Override the string tokenization step while preserving the
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1821:        will be removed from the resulting tokens.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1828:    token_pattern : str, default=r"(?u)\\b\\w\\w+\\b"
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1829:        Regular expression denoting what constitutes a "token", only used
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1830:        if ``analyzer == 'word'``. The default regexp selects tokens of 2
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1832:        and always treated as a token separator).
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1834:        If there is a capturing group in token_pattern then the
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1835:        captured group content, not the entire match, becomes the token.
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1961:        tokenizer=None,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1964:        token_pattern=r"(?u)\b\w\w+\b",
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1984:            tokenizer=tokenizer,
venv\Lib\site-packages\sklearn\feature_extraction\text.py:1987:            token_pattern=token_pattern,
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:110:        tokenizer=None,
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:112:        token_pattern=r"(?u)\b\w\w+\b",
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:127:        self.tokenizer = tokenizer
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:130:        self.token_pattern = token_pattern
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:474:                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:475:                tokenizer=None,
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:494:                strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
venv\Lib\site-packages\sklearn\utils\tests\test_pprint.py:495:                tokenizer=None,
venv\Lib\site-packages\threadpoolctl-3.6.0.dist-info\METADATA:377:  allow password authentication anymore, the username needs to be changed to the
venv\Lib\site-packages\threadpoolctl-3.6.0.dist-info\METADATA:378:  generic name `__token__`:
venv\Lib\site-packages\threadpoolctl-3.6.0.dist-info\METADATA:381:FLIT_USERNAME=__token__ flit publish
venv\Lib\site-packages\threadpoolctl-3.6.0.dist-info\METADATA:384:  and a PyPI token has to be passed in place of the password.
venv\Lib\site-packages\urllib3\connection.py:403:                f"Method cannot contain non-token characters {method!r} (found at least {match.group()!r})"
venv\Lib\site-packages\urllib3\connection.py:647:        key_password: str | None = None,
venv\Lib\site-packages\urllib3\connection.py:662:        self.key_password = key_password
venv\Lib\site-packages\urllib3\connection.py:688:        key_password: str | None = None,
venv\Lib\site-packages\urllib3\connection.py:717:        self.key_password = key_password
venv\Lib\site-packages\urllib3\connection.py:807:                key_password=self.key_password,
venv\Lib\site-packages\urllib3\connection.py:884:            key_password=None,
venv\Lib\site-packages\urllib3\connection.py:910:    key_password: str | None,
venv\Lib\site-packages\urllib3\connection.py:979:        key_password=key_password,
venv\Lib\site-packages\urllib3\connectionpool.py:972:    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
venv\Lib\site-packages\urllib3\connectionpool.py:994:        key_password: str | None = None,
venv\Lib\site-packages\urllib3\connectionpool.py:1020:        self.key_password = key_password
venv\Lib\site-packages\urllib3\connectionpool.py:1073:            key_password=self.key_password,
venv\Lib\site-packages\urllib3\contrib\emscripten\connection.py:173:    key_password: str | None
venv\Lib\site-packages\urllib3\contrib\emscripten\connection.py:207:        key_password: str | None = None,
venv\Lib\site-packages\urllib3\contrib\emscripten\connection.py:223:        self.key_password = key_password
venv\Lib\site-packages\urllib3\contrib\emscripten\connection.py:246:        key_password: str | None = None,
venv\Lib\site-packages\urllib3\contrib\pyopenssl.py:484:        password: str | None = None,
venv\Lib\site-packages\urllib3\contrib\pyopenssl.py:488:            if password is not None:
venv\Lib\site-packages\urllib3\contrib\pyopenssl.py:489:                if not isinstance(password, bytes):
venv\Lib\site-packages\urllib3\contrib\pyopenssl.py:490:                    password = password.encode("utf-8")  # type: ignore[assignment]
venv\Lib\site-packages\urllib3\contrib\pyopenssl.py:491:                self._ctx.set_passwd_cb(lambda *_: password)
venv\Lib\site-packages\urllib3\contrib\socks.py:14:- Usernames and passwords for the SOCKS proxy
venv\Lib\site-packages\urllib3\contrib\socks.py:31:When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
venv\Lib\site-packages\urllib3\contrib\socks.py:32:of the ``proxy_url`` will be sent as the username/password to authenticate
venv\Lib\site-packages\urllib3\contrib\socks.py:37:    proxy_url="socks5h://<username>:<password>@proxy-host"
venv\Lib\site-packages\urllib3\contrib\socks.py:80:    password: str | None
venv\Lib\site-packages\urllib3\contrib\socks.py:116:                proxy_password=self._socks_options["password"],
venv\Lib\site-packages\urllib3\contrib\socks.py:188:        password: str | None = None,
venv\Lib\site-packages\urllib3\contrib\socks.py:195:        if username is None and password is None and parsed.auth is not None:
venv\Lib\site-packages\urllib3\contrib\socks.py:198:                username, password = split
venv\Lib\site-packages\urllib3\contrib\socks.py:221:            "password": password,
venv\Lib\site-packages\urllib3\poolmanager.py:48:    "key_password",
venv\Lib\site-packages\urllib3\poolmanager.py:72:    key_key_password: str | None
venv\Lib\site-packages\urllib3\util\request.py:45:    token = 0
venv\Lib\site-packages\urllib3\util\request.py:48:_FAILEDTELL: Final[_TYPE_FAILEDTELL] = _TYPE_FAILEDTELL.token
venv\Lib\site-packages\urllib3\util\request.py:89:        Colon-separated username:password string for 'authorization: basic ...'
venv\Lib\site-packages\urllib3\util\request.py:93:        Colon-separated username:password string for 'proxy-authorization: basic ...'
venv\Lib\site-packages\urllib3\util\ssl_.py:389:    key_password: str | None = ...,
venv\Lib\site-packages\urllib3\util\ssl_.py:407:    key_password: str | None = ...,
venv\Lib\site-packages\urllib3\util\ssl_.py:424:    key_password: str | None = None,
venv\Lib\site-packages\urllib3\util\ssl_.py:445:    :param key_password:
venv\Lib\site-packages\urllib3\util\ssl_.py:446:        Optional password if the keyfile is encrypted.
venv\Lib\site-packages\urllib3\util\ssl_.py:472:    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
venv\Lib\site-packages\urllib3\util\ssl_.py:473:        raise SSLError("Client private key is encrypted, password is required")
venv\Lib\site-packages\urllib3\util\ssl_.py:476:        if key_password is None:
venv\Lib\site-packages\urllib3\util\ssl_.py:479:            context.load_cert_chain(certfile, keyfile, key_password)
venv\Lib\site-packages\urllib3\util\timeout.py:17:    token = -1
venv\Lib\site-packages\urllib3\util\timeout.py:20:_DEFAULT_TIMEOUT: Final[_TYPE_DEFAULT] = _TYPE_DEFAULT.token
venv\Lib\site-packages\urllib3\util\url.py:182:            print( urllib3.util.Url("https", "username:password",
venv\Lib\site-packages\urllib3\util\url.py:186:            # "https://username:password@host.com:80/path?query#fragment"
venv\Lib\site-packages\urllib3\_base_connection.py:138:        key_password: str | None
venv\Lib\site-packages\urllib3\_base_connection.py:164:            key_password: str | None = None,
venv\Lib\site-packages\werkzeug\datastructures\auth.py:25:    Depending on the auth scheme, either :attr:`parameters` or :attr:`token` will be
venv\Lib\site-packages\werkzeug\datastructures\auth.py:26:    set. The ``Basic`` scheme's token is decoded into the ``username`` and ``password``
venv\Lib\site-packages\werkzeug\datastructures\auth.py:33:        The ``token`` parameter and attribute was added to support auth schemes that use
venv\Lib\site-packages\werkzeug\datastructures\auth.py:34:        a token instead of parameters, such as ``Bearer``.
venv\Lib\site-packages\werkzeug\datastructures\auth.py:47:        token: str | None = None,
venv\Lib\site-packages\werkzeug\datastructures\auth.py:56:        """A dict of parameters parsed from the header. Either this or :attr:`token`
venv\Lib\site-packages\werkzeug\datastructures\auth.py:60:        self.token = token
venv\Lib\site-packages\werkzeug\datastructures\auth.py:61:        """A token parsed from the header. Either this or :attr:`parameters` will have a
venv\Lib\site-packages\werkzeug\datastructures\auth.py:85:            and other.token == self.token
venv\Lib\site-packages\werkzeug\datastructures\auth.py:107:                username, _, password = base64.b64decode(rest).decode().partition(":")
venv\Lib\site-packages\werkzeug\datastructures\auth.py:111:            return cls(scheme, {"username": username, "password": password})
venv\Lib\site-packages\werkzeug\datastructures\auth.py:117:        # No = or only trailing =, this is a token.
venv\Lib\site-packages\werkzeug\datastructures\auth.py:127:                f"{self.username}:{self.password}".encode()
venv\Lib\site-packages\werkzeug\datastructures\auth.py:131:        if self.token is not None:
venv\Lib\site-packages\werkzeug\datastructures\auth.py:132:            return f"{self.type.title()} {self.token}"
venv\Lib\site-packages\werkzeug\datastructures\auth.py:150:    Depending on the auth scheme, either :attr:`parameters` or :attr:`token` should be
venv\Lib\site-packages\werkzeug\datastructures\auth.py:151:    set. The ``Basic`` scheme will encode ``username`` and ``password`` parameters to a
venv\Lib\site-packages\werkzeug\datastructures\auth.py:152:    token.
venv\Lib\site-packages\werkzeug\datastructures\auth.py:159:        The ``token`` parameter and attribute was added to support auth schemes that use
venv\Lib\site-packages\werkzeug\datastructures\auth.py:160:        a token instead of parameters, such as ``Bearer``.
venv\Lib\site-packages\werkzeug\datastructures\auth.py:173:        token: str | None = None,
venv\Lib\site-packages\werkzeug\datastructures\auth.py:179:        self._token = token
venv\Lib\site-packages\werkzeug\datastructures\auth.py:198:        """A dict of parameters for the header. Only one of this or :attr:`token` should
venv\Lib\site-packages\werkzeug\datastructures\auth.py:209:    def token(self) -> str | None:
venv\Lib\site-packages\werkzeug\datastructures\auth.py:210:        """A dict of parameters for the header. Only one of this or :attr:`token` should
venv\Lib\site-packages\werkzeug\datastructures\auth.py:213:        return self._token
venv\Lib\site-packages\werkzeug\datastructures\auth.py:215:    @token.setter
venv\Lib\site-packages\werkzeug\datastructures\auth.py:216:    def token(self, value: str | None) -> None:
venv\Lib\site-packages\werkzeug\datastructures\auth.py:217:        """A token for the header. Only one of this or :attr:`parameters` should have a
venv\Lib\site-packages\werkzeug\datastructures\auth.py:222:        self._token = value
venv\Lib\site-packages\werkzeug\datastructures\auth.py:246:        if name in {"_type", "_parameters", "_token", "_on_update"}:
venv\Lib\site-packages\werkzeug\datastructures\auth.py:263:            and other.token == self.token
venv\Lib\site-packages\werkzeug\datastructures\auth.py:290:        # No = or only trailing =, this is a token.
venv\Lib\site-packages\werkzeug\datastructures\auth.py:295:        if self.token is not None:
venv\Lib\site-packages\werkzeug\datastructures\auth.py:296:            return f"{self.type.title()} {self.token}"
venv\Lib\site-packages\werkzeug\datastructures\auth.py:303:                    value = quote_header_value(value, allow_token=False)
venv\Lib\site-packages\werkzeug\debug\shared\debugger.js:42:  params.set("s", SECRET)
venv\Lib\site-packages\werkzeug\debug\tbtools.py:30:          SECRET = "%(secret)s";
venv\Lib\site-packages\werkzeug\debug\tbtools.py:328:        self, evalex: bool, secret: str, evalex_trusted: bool
venv\Lib\site-packages\werkzeug\debug\tbtools.py:348:            "secret": secret,
venv\Lib\site-packages\werkzeug\debug\tbtools.py:442:def render_console_html(secret: str, evalex_trusted: bool) -> str:
venv\Lib\site-packages\werkzeug\debug\tbtools.py:448:        "secret": secret,
venv\Lib\site-packages\werkzeug\debug\__init__.py:290:        self.secret = gen_salt(20)
venv\Lib\site-packages\werkzeug\debug\__init__.py:363:                secret=self.secret,
venv\Lib\site-packages\werkzeug\debug\__init__.py:415:            render_console_html(secret=self.secret, evalex_trusted=is_trusted),
venv\Lib\site-packages\werkzeug\debug\__init__.py:551:            secret = request.args.get("s")
venv\Lib\site-packages\werkzeug\debug\__init__.py:555:            elif cmd == "pinauth" and secret == self.secret:
venv\Lib\site-packages\werkzeug\debug\__init__.py:557:            elif cmd == "printpin" and secret == self.secret:
venv\Lib\site-packages\werkzeug\debug\__init__.py:563:                and self.secret == secret
venv\Lib\site-packages\werkzeug\exceptions.py:306:        " (e.g. a bad password), or your browser doesn't understand"
venv\Lib\site-packages\werkzeug\http.py:26:_token_chars = frozenset(
venv\Lib\site-packages\werkzeug\http.py:139:def quote_header_value(value: t.Any, allow_token: bool = True) -> str:
venv\Lib\site-packages\werkzeug\http.py:140:    """Add double quotes around a header value. If the header contains only ASCII token
venv\Lib\site-packages\werkzeug\http.py:147:    :param allow_token: Disable to quote the value even if it only has token characters.
venv\Lib\site-packages\werkzeug\http.py:165:    if allow_token:
venv\Lib\site-packages\werkzeug\http.py:166:        token_chars = _token_chars
venv\Lib\site-packages\werkzeug\http.py:168:        if token_chars.issuperset(value_str):
venv\Lib\site-packages\werkzeug\http.py:204:    If a value contains non-token characters, it will be quoted.
venv\Lib\site-packages\werkzeug\http.py:246:    If a value contains non-token characters, it will be quoted.
venv\Lib\site-packages\werkzeug\http.py:266:        The ``allow_token`` parameter is removed.
venv\Lib\site-packages\werkzeug\http.py:309:        parse_list_header('token, "quoted value"')
venv\Lib\site-packages\werkzeug\http.py:310:        ['token', 'quoted value']
venv\Lib\site-packages\werkzeug\http.py:403:_parameter_token_value_re = re.compile(r"[\w!#$%&'*+\-.^`|~]+", flags=re.ASCII)
venv\Lib\site-packages\werkzeug\http.py:409:    ([\w!#$%&'*+\-.^`|~]+)  # one or more token chars with percent encoding
venv\Lib\site-packages\werkzeug\http.py:493:            # Value may be a token.
venv\Lib\site-packages\werkzeug\http.py:494:            if (m := _parameter_token_value_re.match(rest)) is not None:
venv\Lib\site-packages\werkzeug\http.py:746:    >>> hs = parse_set_header('token, "quoted value"')
venv\Lib\site-packages\werkzeug\http.py:751:    >>> 'TOKEN' in hs
venv\Lib\site-packages\werkzeug\http.py:756:    HeaderSet(['token', 'quoted value'])
venv\Lib\site-packages\werkzeug\sansio\request.py:484:            :class:`Authorization` is no longer a ``dict``. The ``token`` attribute
venv\Lib\site-packages\werkzeug\sansio\request.py:485:            was added for auth schemes that use a token instead of parameters.
venv\Lib\site-packages\werkzeug\sansio\response.py:519:        string token only.
venv\Lib\site-packages\werkzeug\sansio\response.py:586:            :class:`WWWAuthenticate` is no longer a ``dict``. The ``token`` attribute
venv\Lib\site-packages\werkzeug\sansio\response.py:587:            was added for auth challenges that use a token instead of parameters.
venv\Lib\site-packages\werkzeug\security.py:7:import secrets
venv\Lib\site-packages\werkzeug\security.py:30:    return "".join(secrets.choice(SALT_CHARS) for _ in range(length))
venv\Lib\site-packages\werkzeug\security.py:33:def _hash_internal(method: str, salt: str, password: str) -> tuple[str, str]:
venv\Lib\site-packages\werkzeug\security.py:36:    password_bytes = password.encode()
venv\Lib\site-packages\werkzeug\security.py:52:                password_bytes, salt=salt_bytes, n=n, r=r, p=p, maxmem=maxmem
venv\Lib\site-packages\werkzeug\security.py:73:                hash_name, password_bytes, salt_bytes, iterations
venv\Lib\site-packages\werkzeug\security.py:81:def generate_password_hash(
venv\Lib\site-packages\werkzeug\security.py:82:    password: str, method: str = "scrypt", salt_length: int = 16
venv\Lib\site-packages\werkzeug\security.py:84:    """Securely hash a password for storage. A password can be compared to a stored hash
venv\Lib\site-packages\werkzeug\security.py:85:    using :func:`check_password_hash`.
venv\Lib\site-packages\werkzeug\security.py:97:    users with a link to reset their password.
venv\Lib\site-packages\werkzeug\security.py:99:    :param password: The plaintext password.
venv\Lib\site-packages\werkzeug\security.py:116:    h, actual_method = _hash_internal(method, salt, password)
venv\Lib\site-packages\werkzeug\security.py:120:def check_password_hash(pwhash: str, password: str) -> bool:
venv\Lib\site-packages\werkzeug\security.py:121:    """Securely check that the given stored password hash, previously generated using
venv\Lib\site-packages\werkzeug\security.py:122:    :func:`generate_password_hash`, matches the given password.
venv\Lib\site-packages\werkzeug\security.py:126:    may contact users with a link to reset their password.
venv\Lib\site-packages\werkzeug\security.py:128:    :param pwhash: The hashed password.
venv\Lib\site-packages\werkzeug\security.py:129:    :param password: The plaintext password.
venv\Lib\site-packages\werkzeug\security.py:139:    return hmac.compare_digest(_hash_internal(method, salt, password)[0], hashval)
venv\Lib\site-packages\werkzeug\test.py:243:        ``Authorization`` header value. A ``(username, password)`` tuple
venv\Lib\site-packages\werkzeug\test.py:361:                    "basic", {"username": auth[0], "password": auth[1]}
venv\Lib\site-packages\werkzeug\urls.py:104:        if parts.password:
venv\Lib\site-packages\werkzeug\urls.py:105:            password = _unquote_user(parts.password)
venv\Lib\site-packages\werkzeug\urls.py:106:            auth = f"{auth}:{password}"
venv\Lib\site-packages\werkzeug\urls.py:159:        if parts.password:
venv\Lib\site-packages\werkzeug\urls.py:160:            password = quote(parts.password, safe="%!$&'()*+,;=")
venv\Lib\site-packages\werkzeug\urls.py:161:            auth = f"{auth}:{password}"
venv\Lib\site-packages\werkzeug\utils.py:199:    >>> secure_filename("../../../etc/passwd")
venv\Lib\site-packages\werkzeug\utils.py:200:    'etc_passwd'
venv\Scripts\activate.bat:4:for /f "tokens=2 delims=:." %%a in ('"%SystemRoot%\System32\chcp.com"') do (
